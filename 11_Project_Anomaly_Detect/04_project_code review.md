👉normalize

정규화의 장점 : optimal solution으로의 수렴 속도가 빠르다

normalize의 종류 : ..

https://sonsnotation.blogspot.com/2020/11/8-normalization.html



👉timm

https://jjuon.tistory.com/19

👉glob

👉numpy array 계산

http://taewan.kim/post/numpy_sum_axis/



👉신경망 모델 정의

https://anweh.tistory.com/21

👉gc : Garbage Collector
gc.collect() : 메모리 관리 작업을 수행, 메모리 사용을 마쳤을 때 메모리를 비움

https://docs.python.org/ko/3.7/library/gc.html

https://medium.com/dmsfordsm/garbage-collection-in-python-777916fd3189

👉torch.cuda.amp.autocast() : autocasting을 위한 모듈 제공torch.cuda.amp.GradScaler() : 
    backward 시 underflow방지를 위해 float32범위로 스케일
    옵티마이저가 파라미터를 업데이트하기전에 원래 스케일대로 복구

**automatic mixed precision package** 가 뭐지

https://aimaster.tistory.com/83

👉optimizer.zero_grad() : 이상적인 학습을 위한 backward()를 호출할 때 하는 초기설정

https://algopoolja.tistory.com/55

👉eval(), torch.no_grad()

https://bluehorn07.github.io/2021/02/27/model-eval-and-train.html
Machine learning

# Machine learning : Binary Classification



conda install mglearn : ì‚°í¬ë„ ê·¸ë˜í”„ë¡œ ê³¼ì •ì„ í™•ì¸í•˜ê¸° ì‰¬ì›€



### 01. Logistic Regression

- Classificationì—ëŠ” ì´í•­ë¶„ë¥˜(binary classification), ë‹¤í•­ë¶„ë¥˜(Multinomial classification)ê°€ ìˆë‹¤.

- Logistic Regressionì€ Linear Regressionì„ í™•ì¥í•œ ë¶„ë¥˜ì´ë‹¤. 

- Logistic Regressionì€ Deep Learningìœ¼ë¡œ ì—°ê²°ë˜ëŠ” ê¸°ë³¸ component ì´ë‹¤.

#### A) ê°œìš”

> ğŸ“Œ ì´ˆê¸° ì¸ê³µì§€ëŠ¥ ì•Œê³ ë¦¬ì¦˜
>
> **Perceptron** ì€ ì´ˆê¸° ì¸ê³µì§€ëŠ¥ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ, ê²°ê³¼ê°’ zë¥¼ step function(ê³„ë‹¨í•¨ìˆ˜)ë¥¼ ì‚¬ìš©í•˜ì—¬ 0ë³´ë‹¤ í¬ë©´ 1, 0ë³´ë‹¤ ì‘ìœ¼ë©´ -1ë¡œ ë‘ê°œì˜ ê°’ìœ¼ë¡œ ë¶„ë¥˜í–ˆë˜ ì•Œê³ ë¦¬ì¦˜ì´ë‹¤.
>
> ê·¸ëƒ¥ ì´ˆê¸° ì•Œê³ ë¦¬ì¦˜ì´ë¼ëŠ”ê±°ì§€, ì´ê²Œ í™•ì¥ë˜ì–´ì„œ ì§€ê¸ˆì˜ ì•Œê³ ë¦¬ì¦˜ì´ ë˜ì—ˆë‹¤ë˜ê±°ëŠ” ì•„ë‹ˆë‹¤.

![image-20220405222109002](../img/image-20220405222109002.png)

Logistic Regressionì€ Linear Regressionì— í™œì„±í™” í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ 0ê³¼ 1ì‚¬ì´ì˜ í™•ë¥ ê°’ aë¥¼ ê²°ê³¼ë¡œ ë§Œë“ ë‹¤.

ê·¸ë¦¬ê³  ì´ ê²°ê³¼ê°’ì„ ì„ê³„í•¨ìˆ˜ë¥¼ í†µí•´ ë¶„ë¥˜í•˜ì—¬ ìµœì¢… ê²°ê³¼ê°’ì„ ë§Œë“¤ê²Œ ëœë‹¤.

> ğŸ‘‰ Logistic Regression ì˜ˆì‹œ
>
> ê³µì¥ìƒì‚° ì œí’ˆ ì •ìƒ/ë¶ˆëŸ‰ íŒë… , CT ì‚¬ì§„ìœ¼ë¡œ ì•”/ì •ìƒ íŒë…, 
> ì£¼ì‹ ì½”ì¸ ë“±ì´ ë‹¤ìŒë‚  ì˜¤ë¥¼ì§€/ë‚´ë¦´ì§€, ì‹ ìš©ì¹´ë“œì˜ ìƒíƒœê°€ ì •ìƒì‚¬ìš©ìƒíƒœ/ë„ë‚œì¹´ë“œ íŒë…

<img src="../img/image-20220405222742705.png" width="700" height="400">

ë‹¤ìŒì€ mglearnì—ì„œ ì œê³µí•˜ëŠ” datasetì„ scatterê·¸ë˜í”„ë¡œ í‘œí˜„í•˜ê³ , Linear Regression ì„ ì„ ê·¸ë¦°ê²ƒì´ë‹¤.

1. Linear Regressionì„ ì´ìš©í•´ì„œ Training Data Setì˜ íŠ¹ì„±ê³¼ ë¶„í¬ë¥¼ ì˜ í‘œí˜„í•˜ëŠ” ì§ì„ ì„ ì°¾ëŠ”ë‹¤. 
2. ì´ ê²½ê³„ì„ ì„ ê¸°ì¤€ìœ¼ë¡œ 0ê³¼ 1ë¡œ ë¶„ë¥˜ëœë‹¤.

ì´ ë°©ë²•ì€ ì •í™•ë„ê°€ ìƒë‹¹íˆ ë†’ì•„ Deep Learningì˜ ê¸°ë³¸ componentë¡œ ì‚¬ìš©ëœë‹¤.

> â“ ê·¸ëŸ¼ ì´ì§„ë¶„ë¥˜ë¬¸ì œë¥¼ Logistic Regressionì´ ì•„ë‹Œ Linear Regressionìœ¼ë¡œ ë¶„ë¥˜í•˜ë©´ ë˜ì§€ ì•Šì„ê¹Œ?
>
> ì´ìƒì¹˜ ë“±ì˜ ë¬¸ì œì—ì„œëŠ” Linear Regressionìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ì—†ëŠ” ë¬¸ì œê°€ ìˆë‹¤.
>
> ì˜ˆë¥¼ë“¤ì–´ ê³µë¶€ì‹œê°„ì— ë”°ë¥¸ í•©ê²©ë¥ ì„ ë”°ì¡Œì„ ë•Œ 100ì‹œê°„ì„ ê³µë¶€í•œ ì´ìƒì¹˜ê°€ ì¡´ì¬í•œë‹¤ë©´, ì¶©ë¶„íˆ í•©ê²©í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ë¥¼ ì˜ëª» ë¶„ë¥˜ í•  ìˆ˜ ìˆë‹¤.

#### B) Cross Entropy

ì•ì„œ Logistic Regressionì€ Linear Regressionì— í™œì„±í™”í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ ê²°ê³¼ë¥¼ ì¶”ì¶œí•œë‹¤ê³  í•˜ì˜€ë‹¤.

Logistic Regressionì—ì„œëŠ” ì´ í™œì„±í™” í•¨ìˆ˜ë¡œ <span style="color:red">Sigmoid</span> í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤.
$$
\text Sigmoid = {1\over 1+e^{-x}}
$$
<img src="../img/image-20220405224036593.png" width="300" height="200">

Sigmoidì˜ ê·¸ë˜í”„ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê·¸ë˜í”„ë¡œ ê·¸ë ¤ì§€ë©°, ì´í•¨ìˆ˜ë¥¼ ì ìš©í•˜ë©´ 0ê³¼ 1ì‚¬ì´ì˜ ì‹¤ìˆ˜ë¡œ ë³€í™˜ëœë‹¤.

<span style="color:red">Linear Regression ê³µì‹ì— Sigmoidë¥¼ ì ìš©</span>í•˜ê³ , MSEë¥¼ ì‚¬ìš©í•œ loss functionì„ ë§Œë“¤ë©´ ë‹¤ìŒê³¼ ê°™ì•„ì§„ë‹¤.
$$
\text {Linear Regression Model :} \quad \hat y=Wx+b \qquad\qquad\qquad\qquad
\\
\text {Linear Regression loss func :} \quad E(W,b)={1\over n}\sum_{i=1}^n[t_{i}-(wx_{i}+b)]^2
\\
\\
\text {Logistic Regression Model :} \quad \hat y = {1\over 1+e^{-(Wx+b)}} \qquad\qquad\qquad
\\
\qquad\qquad\qquad\quad\text {New loss func :} \quad E(W,b) = {1\over n}\sum_{i=1}^n\Big[t_i -({1\over e^{(Wx_i+b)}}) \Big]^2
$$

<img src="../img/image-20220405225751373.png" width="300" height="200">

ê·¸ë˜í”„ë¥¼ ë³´ë©´ Convex(ë³¼ë¡)í•œ í˜•íƒœê°€ ì•„ë‹Œê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. 

ì§€ìˆ˜í•¨ìˆ˜(e<sup>-x</sup>)ì˜ íŠ¹ì„±ìœ¼ë¡œ ëª¨ì–‘ì´ êµ¬ë¶ˆêµ¬ë¶ˆí•˜ë‹¤.

ì´ëŒ€ë¡œëŠ” MSEë¥¼ ì´ìš©í•œ loss functionì„ ì‚¬ìš©í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— ì‹ì— <span style="color:red">Log</span>ë¥¼ ì·¨í•´ì£¼ëŠ” ë°©ì‹ì„ ì„ íƒí•œë‹¤.

$$
\text {Logistic Regression loss function :} \quad
=-\sum_{i=1}^n \Big\{t_i \log y_i + (1-t_i) \log  (1-y_i) \Big\}
$$

> log ê³µì‹ì„ ì‚¬ìš©í•˜ë ¤ë©´ c=log<sub>b</sub>a ê³µì‹ì—ì„œ aâ‰ 0ì˜ ì¡°ê±´ì´ í•„ìš”í•˜ë‹¤.
>
> ê·¸ë˜ì„œ í˜¹ì‹œë‚˜ 0ì´ ë“¤ì–´ì˜¬ ê²ƒì„ ëŒ€ë¹„í•´ í”„ë¡œê·¸ë¨ì ìœ¼ë¡œ ê³„ì‚°í•´ì£¼ê¸° ìœ„í•´ ì•½ê°„ì˜ delta ê°’ì„ ì£¼ê²Œ ëœë‹¤.

ì´ ì‹ì„ <span style="background-color:#fff5b1;">Cross Entropy</span>  ë˜ëŠ” log lossë¼ê³  ë¶€ë¥¸ë‹¤.

### 02. Logistic Regression êµ¬í˜„

#### A) Python

```python
# Python êµ¬í˜„

import numpy as np

########## ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì— ëŒ€í•œ ìˆ˜ì¹˜ë¯¸ë¶„ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ ########

def numerical_derivative(f, x):
    
    delta_x = 1e-4
    derivative_x = np.zeros_like(x) 
    it = np.nditer(x, flags=['multi_index'])
    
    while not it.finished:
        
        idx = it.multi_index
        tmp = x[idx]
    
        x[idx] = tmp + delta_x
        fx_plus_delta = f(x)
        
        x[idx] = tmp -delta_x
        fx_minus_delta = f(x)
        
        derivative_x[idx] = (fx_plus_delta - fx_minus_delta) / (2*delta_x)
        
        x[idx] = tmp
        it.iternext()
        
    return derivative_x

################ ìˆ˜ì¹˜ë¯¸ë¶„ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ ë ###########


# Training Data Set
x_data = np.array([2,4,6,8,10,12,14,16,18,20]).reshape(-1,1)
t_data = np.array([0,0,0,0,0,0,1,1,1,1]).reshape(-1,1)

# Weight, bias ì •ì˜
W = np.random.rand(1,1)
b = np.random.rand(1)

# Logiscit Regression model, predict model, hypothesis
def predict(x):
    z = np.dot(x,W)+b               # linear regression model
    y = 1 / (1 + np.exp(-1 * z))   # logistic regression model
    result = 0
    # ê³„ì‚°ë˜ëŠ” y ê°’ì€ ê²°êµ­ 0ê³¼ 1ì‚¬ì´ì˜ í™•ë¥ ê°’
    if y >= 0.5:
        result = 1
    else : 
        result = 0
    return y,result

# Cross Entropy(log loss)
def loss_func(input_data):  # [W,b]
    
    input_W = input_data[:-1].reshape(-1,1)
    input_b = input_data[-1]
    
    z = np.dot(x_data, input_W) + input_b
    y = 1 / (1 + np.exp(-1 *z))
    
    delta = 1e-7
    
    # cross entropy
    return -1 * np.sum(t_data*np.log(y+delta)+(1-t_data)*np.log(1-y+delta))

# learning_rate
learning_rate = 1e-4

# ë°˜ë³µí•™ìŠµ
for step in range(300000):
    input_param = np.concatenate((W.ravel(), b.ravel()), axis=0) # [W b]
    derivate_result = learning_rate * numerical_derivative(loss_func, input_param)
    
    W = W - derivate_result[:-1].reshape(-1,1)
    b = b - derivate_result[-1]
    
    if step % 30000 == 0 :
        input_param = np.concatenate((W.ravel(), b.ravel()), axis=0)
        print('W:{}, b:{}, loss:{}'.format(W, b, loss_func(input_param)))  
```

```python
# prediction
study_hour = np.array([[13]])
y_prob, result = predict(study_hour)
print('í•©ê²©í™•ë¥ :{}, í•©ê²©ì—¬ë¶€:{}'.format(y_prob, result))
# í•©ê²©í™•ë¥ :[[0.54438019]], í•©ê²©ì—¬ë¶€:1
```

#### B) sklearn

```python
# sklearnìœ¼ë¡œ êµ¬í˜„
from sklearn import linear_model
ã… 
x_data = np.array([2, 4, 6, 8,10,12,14,16,18,20]).reshape(-1,1)
t_data = np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1])

model = linear_model.LogisticRegression()

model.fit(x_data, t_data)

study_hour = np.array([[13]])

result = model.predict(study_hour) # ìµœì¢…ê²°ê³¼ë§Œ ì•Œë ¤ì¤€ë‹¤.
result_prob = model.predict_proba(study_hour)

print('í•©ê²©í™•ë¥ :{}, í•©ê²©ì—¬ë¶€:{}'.format(result_prob, result))
# í•©ê²©í™•ë¥ :[[0.50009391 0.49990609]], í•©ê²©ì—¬ë¶€:[0]
```

#### C) Tensorflow

```python
# Tensorflow êµ¬í˜„

import tensorflow as tf

x_data = np.array([2, 4, 6, 8,10,12,14,16,18,20]).reshape(-1,1)
t_data = np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1]).reshape(-1,1)

# placeholdeer
X = tf.placeholder(shape=[None,1], dtype=tf.float32)
T = tf.placeholder(shape=[None,1], dtype=tf.float32)

# Weight & bias
W = tf.Variable(tf.random.normal([1,1]))
b = tf.Variable(tf.random.normal([1]))

# Model(Hypothesis)
logit = tf.matmul(X,W) + b # linear regression model
H = tf.sigmoid(logit)      # logistic regression model

# loss function
loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,
                                                              labels=T))

# train
train = tf.train.GradientDescentOptimizer(learning_rate=1e-3).minimize(loss)

# Session & ì´ˆê¸°í™”
sess = tf.Session()
sess.run(tf.global_variables_initializer())

# ë°˜ë³µí•™ìŠµ
for step in range(30000):
    _, W_val, b_val, loss_val = sess.run([train, W, b, loss],
                                         feed_dict={X: x_data,
                                                    T: t_data})
    if step % 3000 ==0:
        print('W:{}, b:{}, loss:{}'.format(W_val, b_val, loss_val))
```

```python
# prediction
study_hour = np.array([[13]]) # 12ì‹œê°„ì€ ë¶ˆí•©ê²©ì´ì—ˆê³ , 14ì‹œê°„ì€ í•©ê²©ì´ì—ˆì–´ìš”!
result = sess.run(H, feed_dict={X: study_hour})
print('í•©ê²©í™•ë¥ :{}'.format(result))
# í•©ê²©í™•ë¥ :[[0.58296657]]
```



### 03. ì˜ˆì œ

#### ì˜ˆì œ1

admission.csv

í•©ê²©, ë¶ˆí•©ê²©ì—¬ë¶€ íŒë³„

```python
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn import linear_model
from sklearn.preprocessing import MinMaxScaler # ì •ê·œí™”
from scipy import stats # ì´ìƒì¹˜ ì²˜ë¦¬

import matplotlib.pyplot as plt

import warnings
warnings.filterwarnings(action='ignore')



# Raw Data Loading
df = pd.read_csv('./data/admission.csv')

#####################################
# preprocessing
#####################################

# ê²°ì¸¡ì¹˜
# df.isnull().sum() # ì—†ìŒ

# ì´ìƒì¹˜
# ì¢…ì†ë³€ìˆ˜ì˜ ì´ìƒì¹˜ë¥¼ outlier
# ë…ë¦½ë³€ìˆ˜ì˜ ì´ìƒì¹˜ë¥¼ ì§€ëŒ€ê°’
# 1. ëˆˆìœ¼ë¡œ ì‰½ê²Œ í™•ì¸í•˜ëŠ” ê°€ì¥ ì‰¬ìš´ ë°©ë²• : botplot
    
# figure = plt.figure()
    
# ax1 = figure.add_subplot(1,4,1)
# ax2 = figure.add_subplot(1,4,2)
# ax3 = figure.add_subplot(1,4,3)
# ax4 = figure.add_subplot(1,4,4)
# ax1.set_title('admit')
# ax2.set_title('GRE')
# ax3.set_title('GPA')
# ax4.set_title('RANK')

# ax1.boxplot(df['admit'])
# ax2.boxplot(df['gre'])
# ax3.boxplot(df['gpa'])
# ax4.boxplot(df['rank'])

# plt.tight_layout()
# plt.show()

# boxpltìœ¼ë¡œ í™•ì¸í•´ë³´ë‹ˆ ì´ìƒì¹˜ ì¡´ì¬
# z-scoreì„ ì´ìš©í•´ì„œ ì´ìƒì¹˜ë¥¼ ì œê±°í•˜ê³  ì§„í–‰

zscore_threshold = 2.0

for col in df.columns:
    outlier = df[col][np.abs(stats.zscore(df[col])) > zscore_threshold]
    df = df.loc[~df[col].isin(outlier)]

# ì •ê·œí™”

x_data = df.drop('admit', axis=1)
t_data = df['admit'].values.reshape(-1,1)
# t_dataëŠ” 0ê³¼ 1ë¡œë§Œ êµ¬ì„±ë˜ì–´ìˆì–´ìš”. ë”°ë¼ì„œ ì •ê·œí™” í•„ìš”ì—†ìŒ

scaler = MinMaxScaler()
scaler.fit(x_data)

norm_x_data = scaler.transform(x_data)
# print(norm_x_data)

# training data set
# norm_x_data
# t_data

### sklearn êµ¬í˜„

model = linear_model.LogisticRegression()

model.fit(x_data, t_data)

my_score = np.array([[600, 3.8, 1]])
predict_val = model.predict(my_score)
predict_proba = model.predict_proba(my_score)

print(f'sklearnì˜ ê²°ê³¼ í•©ê²©ì—¬ë¶€ : {predict_val}, í™•ë¥  : {predict_proba}')


##################################
# tensorflow êµ¬í˜„

# training data set
# norm_x_data
# t_data

# placeholder
X = tf.placeholder(shape=[None,3], dtype = tf.float32)
T = tf.placeholder(shape=[None,1], dtype = tf.float32)

# Weight & bias
W = tf.Variable(tf.random.normal([3,1]))
b = tf.Variable(tf.random.normal([1]))

# Hypothesis, model, predict model, logistic regression model
logit = tf.matmul(X,W)+b
H = tf.sigmoid(logit)

# loss function, cross entropy, loglossë¼ê³  í•˜ê¸°ë„ í•´ìš”!
loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,
                                                              labels=T))

# train
train = tf.train.GradientDescentOptimizer(learning_rate=1e-4).minimize(loss)

# Session, ì´ˆê¸°í™”
sess = tf.Session()
sess.run(tf.global_variables_initializer())

# ë°˜ë³µí•™ìŠµ
for step in range(300000):
    _, loss_val = sess.run([train, loss], feed_dict={X: norm_x_data,
                                                 T: t_data})
    if step%30000 == 0:
        print(f'lossì˜ ê°’ : {loss_val}')


```

```python
# predict
my_score = np.array([[600, 3.8, 1]])
norm_my_score = scaler.transform(my_score)

result = sess.run(H, feed_dict={X : norm_my_score})
print(f'tensorflowë¡œ ì˜ˆì¸¡í•œ ê²°ê³¼ : {result}')
# tensorflowë¡œ ì˜ˆì¸¡í•œ ê²°ê³¼ëŠ” íƒˆë½
```

```python
# Regressionì˜ Matricsë¥¼ ì•Œì•„ë³´ì•„ìš”

# Ozone 

import numpy as np
import pandas as pd
from sklearn import linear_model
from scipy import stats
from sklearn.model_selection import train_test_split

df = pd.read_csv('./data/ozone.csv')

# ê²°ì¸¡ì¹˜ì œê±°
training_data = df.dropna(how='any', inplace=False)

# ì´ìƒì¹˜ì œê±°
for col in training_data.columns:
    outlier = training_data[col][np.abs(stats.zscore(training_data[col])) > zscore_threshold]
    training_data = training_data.loc[~training_data[col].isin(outlier)]
    
# ì •ê·œí™” x

# Data Set
x_data = training_data[['Solar.R', 'Wind','Temp']].values
t_data = training_data['Ozone'].values.reshape(-1,1)

# Train / Validation Data Set
train_x_data, valid_x_data, train_t_data, valid_t_data = \
train_test_split(x_data,
                 t_data,
                 test_size=0.3,
                 random_state=2) # randomì˜ seed ì—­í• 

# Model
model = linear_model.LinearRegression()

# Model í•™ìŠµ
model.fit(train_x_data, train_t_data)

# ì˜ˆì¸¡ê°’(predict_value)
# ì •ë‹µ(predict_value)
predict_value = model.predict(valid_x_data)
```

#### ì˜ˆì œ 2

ìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°

- sklearnì—ì„œ ì œê³µí•˜ëŠ” ìœ ë°©ì•” ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ í•™ìŠµ

- Hold-Out validation ë°©ì‹ê³¼ K-Fold cross validation ë°©ì‹ì„ ë¹„êµ
  ğŸ‘‰ ë‘˜ë‹¤ í•´ë³´ê³  ë” ë‚˜ì€ê²ƒìœ¼ë¡œ í•˜ëŠ”ê±´ê°€ë´ìš”!
- Tensorflowë¡œ Logistic Regressionêµ¬í˜„, ì •í™•ë„ ì¸¡ì •

```python
# ìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ë¥¼ ê°€ì§€ê³  êµ¬í˜„í•´ë³´ì•„ìš”!
# ì´ ë°ì´í„°ëŠ” sklearnì´ ì œê³µí•˜ëŠ” ë°ì´í„°ë¥¼ ì‚¬ìš©í•  êº¼ì˜ˆìš”!
# sklearnê³¼ tensorflowë¥¼ ì´ìš©í•´ì„œ êµ¬í˜„í•´ ë³´ì•„ìš”!

import numpy as np
from sklearn import linear_model   # LogisticRegression()
from sklearn.datasets import load_breast_cancer  # ë°ì´í„° ë¡œë”©í•˜ê¸° ìœ„í•œ í•¨ìˆ˜
from sklearn.model_selection import train_test_split  # í•™ìŠµë°ì´í„°ì™€ í‰ê°€ë°ì´í„° ë¶„ë¦¬
from sklearn.model_selection import cross_val_score  # cross validationí•˜ê¸° ìœ„í•´ í•„ìš”

import warnings
warnings.filterwarnings(action='ignore')

# Raw Data Loading
cancer = load_breast_cancer()
# print(type(cancer))   # <class 'sklearn.utils.Bunch'>
                      # sklearnì´ ë°ì´í„°ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ìë£Œêµ¬ì¡°.
                      # pythonì˜ dictionaryì™€ ìœ ì‚¬í•œ êµ¬ì¡°.
# print(cancer)      
# dataë¼ëŠ” ì†ì„±ê³¼ targetì´ë¼ëŠ” ì†ì„±ì„ ê°€ì§€ê³  ìˆê³ 
# dataë¼ëŠ” ì†ì„±ì´ ë…ë¦½ë³€ìˆ˜, targetì´ ì¢…ì†ë³€ìˆ˜
# print(cancer.data.shape, cancer.target.shape)  # (569, 30) (569,)

# print(np.unique(cancer.target, return_counts=True))  
# array([0, 1]), array([212, 357]
# print(cancer.DESCR)  # ìœ ë°©ì•” ë°ì´í„°ì— ëŒ€í•œ ìƒì„¸ ë‚´ìš©!
# :Missing Attribute Values: None
# :Class Distribution: 212 - Malignant(ì•…ì„±), 357 - Benign(ì •ìƒ)

# Data Set
x_data = cancer.data
t_data = cancer.target


# Hold-out validationì„ ìœ„í•´ì„œ trainê³¼ validationë°ì´í„°ë¥¼ ë¶„ë¦¬
train_x_data, test_x_data, train_t_data, test_t_data = \
train_test_split(x_data,
                 t_data,
                 test_size=0.2,
                 random_state=2,
                 stratify=t_data)

model = linear_model.LogisticRegression()

# K-Fold cross validation
test_score = cross_val_score(model, x_data, t_data, scoring='accuracy', cv=5)
print(test_score)
print(test_score.mean())

# Hold-out ë°©ì‹ìœ¼ë¡œ validation
model.fit(train_x_data, train_t_data)
test_score = model.score(test_x_data, test_t_data)
print(test_score)

# ì—¬ê¸°ê¹Œì§€ê°€ sklearnìœ¼ë¡œ êµ¬í˜„í•œê²ƒ
```

```python
# Tensorflowë¥¼ ì´ìš©í•´ êµ¬í˜„í•´ë³´ì•„ìš”
import tensorflow as tf

## tensorflow ê·¸ë˜í”„

## placeholder
X = tf.placeholder(shape=[None,30], dtype=tf.float32)
T = tf.placeholder(shape=[None,1], dtype=tf.float32)

# Weight & bias
W = tf.Variable(tf.random.normal([30,1]))
b = tf.Variable(tf.random.normal([1]))

# Hypothesis, model, predict model, Logistic Regression
logit = tf.matmul(X,W)+b
H = tf.sigmoid(logit)

# cross entropy(loss function)
loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,
                                                              labels = T))

# train
train = tf.train.GradientDescentOptimizer(learning_rate=1e-4).minimize(loss)

# Session ì´ˆê¸°í™”
sess = tf.Session()
sess.run(tf.global_variables_initializer()) # ì´ˆê¸°í™” ì‘ì—…

# ë°˜ë³µí•™ìŠµ
# ì „ì²´ ë°ì´í„°ë¥¼ ì´ìš©í•´ì„œ 1ë²ˆ í•™ìŠµí•˜ëŠ”ê²ƒì„ => 1 epoch(ì—í­)
for step in range(100000):
    _, loss_val = sess.run([train, loss], feed_dict={X: train_x_data,
                                                     T: train_t_data.reshape(-1,1)})
    if step % 10000 ==0:
        print(f'loss value : {loss_val}')

```

```python
# ì •í™•ë„ ì¸¡ì •

# validation data(test_x_data, test_t_data)ë¥¼ ì´ìš©í•´ì„œ ì •í™•ë„ë¥¼ ì¸¡ì •
predict = tf.cast(H >= 0.5, dtype=tf.float32) # TrueëŠ” 1ë¡œ, FalseëŠ” 0ìœ¼ë¡œ ë°”ê¿”ì¤€ë‹¤.

correct = tf.equal(predict, T) # True, False, True....
accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32)) # 1, 0, 1... ë‚˜ì˜¨ê±° mean() í•´ì„œ accuracy

accuracy_val = sess.run(accuracy, feed_dict={X: test_x_data,
                                             T: test_t_data.reshape(-1,1)})

print(f'Accuracy : {accuracy_val}')  
# Accuracy : 0.8771929740905762
```

#### ì˜ˆì œ 3

- sklearnì„ ì‚¬ìš©í•˜ì—¬ Logistic Regression, SGD Classifier ë¹„êµ
- SGD ClassifierëŠ” ì •ê·œí™”ë¥¼ ê±°ì³ì•¼ í•œë‹¤.
- SGD Classifierì— L2 Regularization(ê·œì œ)ë„ í¬í•¨í•´ì„œ ë¹„êµ : ì¡°ê¸ˆ ë” ë‚˜ì•„ì§„ë‹¤.

```python
# ìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ì„ ì´ìš©í•´ì„œ Logistic Regressionì„ êµ¬í˜„í•´ ë³´ì•„ìš”!

import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Raw Data Set Loading
cancer = load_breast_cancer()

x_data = cancer.data     # 2ì°¨ì› ndarray - ë…ë¦½ë³€ìˆ˜, feature
t_data = cancer.target   # 1ì°¨ì› ndarray - ì¢…ì†ë³€ìˆ˜, label

train_x_data, test_x_data, train_t_data, test_t_data = \
train_test_split(x_data,
                 t_data,
                 test_size=0.3,
                 stratify=t_data,
                 random_state=2)

# Model ìƒì„±
model = linear_model.LogisticRegression()

# Model í•™ìŠµ
model.fit(train_x_data, train_t_data)

# accuracyë¡œ model í‰ê°€
test_score = model.score(test_x_data, test_t_data)

print('Logistic Regression Modelì˜ ì •í™•ë„ : {}'.format(test_score))
# 0.9473684210526315
```

```python
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Raw Data Set Loading
cancer = load_breast_cancer()

x_data = cancer.data     # 2ì°¨ì› ndarray - ë…ë¦½ë³€ìˆ˜, feature
t_data = cancer.target   # 1ì°¨ì› ndarray - ì¢…ì†ë³€ìˆ˜, label

train_x_data, test_x_data, train_t_data, test_t_data = \
train_test_split(x_data,
                 t_data,
                 test_size=0.3,
                 stratify=t_data,
                 random_state=2)

# Model ìƒì„±
sgd = linear_model.SGDClassifier(loss='log',   # logistic regressionì„ ì´ìš©
                                 tol=1e-5,     # ì–¼ë§ˆë‚˜ ë°˜ë³µí• ê±´ì§€ë¥¼ lossê°’ìœ¼ë¡œ ì„¤ì • 
                                 random_state=2)
# Model í•™ìŠµ
sgd.fit(train_x_data, train_t_data)

# Accuracy ì¸¡ì •
test_score = sgd.score(test_x_data, test_t_data)

print('SGDClassifierì˜ ì •í™•ë„ : {}'.format(test_score))
# 0.8947368421052632
# ì™œ ê·¸ëŸ´ê¹Œ...???
# ì •ê·œí™” ì•ˆí–ˆì–´ìš”!  => ê° featureë§ˆë‹¤ scaleì´ ì œê°ê°ì´ì˜ˆìš”!

```

```python
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Raw Data Set Loading
cancer = load_breast_cancer()

x_data = cancer.data     # 2ì°¨ì› ndarray - ë…ë¦½ë³€ìˆ˜, feature
t_data = cancer.target   # 1ì°¨ì› ndarray - ì¢…ì†ë³€ìˆ˜, label

train_x_data, test_x_data, train_t_data, test_t_data = \
train_test_split(x_data,
                 t_data,
                 test_size=0.3,
                 stratify=t_data,
                 random_state=2)

# Data ì •ê·œí™”
scaler = StandardScaler()
scaler.fit(train_x_data)

# Model ìƒì„±
sgd = linear_model.SGDClassifier(loss='log',   # logistic regressionì„ ì´ìš©
                                 tol=1e-5,     # ì–¼ë§ˆë‚˜ ë°˜ë³µí• ê±´ì§€ë¥¼ lossê°’ìœ¼ë¡œ ì„¤ì • 
                                 random_state=2)
# Model í•™ìŠµ
sgd.fit(scaler.transform(train_x_data), train_t_data)

# Accuracy ì¸¡ì •
test_score = sgd.score(scaler.transform(test_x_data), test_t_data)

print('ì •ê·œí™”ë¥¼ ì´ìš©í•œ SGDClassifierì˜ ì •í™•ë„ : {}'.format(test_score))
# 0.9649122807017544

```

```python
# ìœ„ì˜ ì½”ë“œì— L2 Regularization(ê·œì œ)ë„ í¬í•¨í•´ ë³´ì•„ìš”!

import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Raw Data Set Loading
cancer = load_breast_cancer()

x_data = cancer.data     # 2ì°¨ì› ndarray - ë…ë¦½ë³€ìˆ˜, feature
t_data = cancer.target   # 1ì°¨ì› ndarray - ì¢…ì†ë³€ìˆ˜, label

train_x_data, test_x_data, train_t_data, test_t_data = \
train_test_split(x_data,
                 t_data,
                 test_size=0.3,
                 stratify=t_data,
                 random_state=2)

# Data ì •ê·œí™”
scaler = StandardScaler()
scaler.fit(train_x_data)

# Model ìƒì„±
sgd = linear_model.SGDClassifier(loss='log',   # logistic regressionì„ ì´ìš©
                                 tol=1e-5,     # ì–¼ë§ˆë‚˜ ë°˜ë³µí• ê±´ì§€ë¥¼ lossê°’ìœ¼ë¡œ ì„¤ì • 
                                 penalty='l2', # L2 ê·œì œë¥¼ ì´ìš©í• êº¼ì˜ˆìš”! 
                                 alpha=0.001,  # ê·œì œ ê°•ë„     
                                 random_state=2)
# Model í•™ìŠµ
sgd.fit(scaler.transform(train_x_data), train_t_data)

# Accuracy ì¸¡ì •
test_score = sgd.score(scaler.transform(test_x_data), test_t_data)

print('ì •ê·œí™”ë¥¼ ì´ìš©í•œ SGDClassifierì˜ ì •í™•ë„ : {}'.format(test_score))
# 0.9649122807017544
# ê·œì œë¥¼ ì´ìš©í•˜ë©´ ì¡°ê¸ˆ ë” ë‚˜ì€ ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆì–´ìš”!
# 0.9707602339181286
```

#### ì˜ˆì œ 4 : multinomial

- sklearnìœ¼ë¡œ BMI ì˜ˆì¸¡ëª¨ë¸ì„ ë§Œë“¤ì–´ë³´ê³ , í‰ê°€

```python
# BMI ì˜ˆì œ êµ¬í˜„ - sklearnìœ¼ë¡œ ë¨¼ì € êµ¬í˜„í•˜ê³  ì„±ëŠ¥í‰ê°€ë¥¼ ì§„í–‰
# ì„±ëŠ¥í‰ê°€ì˜ matricì€ accuracyë¡œ ì§„í–‰

import numpy as np
import pandas as pd
from sklearn import linear_model
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from scipy import stats
import tensorflow as tf

## Raw Data Loading
df = pd.read_csv('./data/bmi.csv', skiprows=3)
# df.head()
# print(df.shape) # (20000, 3)
```

```python
## Pre-Processing

## Missing Value
# df.isnull().sum() # ê²°ì¸¡ì¹˜ ì—†ìŒ

## Outlier
## zscore ë°©ì‹
zscore_threshold=2.0
# (np.abs(stats.zscore(df['height']))>zscore_threshold).sum() # 0
# (np.abs(stats.zscore(df['weight']))>zscore_threshold).sum() # 0
# np.unique(df['label'], return_counts=True)
# ì´ìƒì¹˜ë„ ì—†ê³  ë°ì´í„°ì˜ í¸í–¥ë„ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤.

## Data Split
train_x_data, test_x_data, train_t_data, test_t_data =\
train_test_split(df[['height','weight']],
                 df['label'],
                 test_size=0.3,
                 random_state=1,
                 stratify=df['label'])

## Normalization
scaler = MinMaxScaler()
scaler.fit(train_x_data)

norm_train_x_data = scaler.transform(train_x_data)
norm_test_x_data = scaler.transform(test_x_data)
```

```python
## Model ìƒì„± í›„ í•™ìŠµ ë° í‰ê°€
# model = linear_model.LogisticRegression()
model = linear_model.LogisticRegression(C=100000)
# (C=1000) # 9.845 ê·œì œí•´ë„ ì¢‹ì•„ì§€ì§€ ì•ŠìŒ
# ê·œì œë¥¼ ì ìš©í•  ìˆ˜ ìˆì–´ìš”(L2 ê·œì œ)
# alphaê°’ì€ ì •í•´ì£¼ì–´ì•¼ í•œë‹¤.
# C = 1 / alpha

model.fit(norm_train_x_data, train_t_data)

## í‰ê°€ë¥¼ ìœ„í•œ ì˜ˆì¸¡ê²°ê³¼ë¥¼ ì–»ì–´ë‚´ìš”
predict_val = model.predict(norm_test_x_data)

acc = accuracy_score(predict_val, test_t_data)
print(f'sklearnìœ¼ë¡œ êµ¬í•œ Accuracy : {acc}') # 0.9851666666666666

# prediction
result = model.predict(scaler.transform(np.array([[187, 81]])))
print(result) # [1]

```

```python
## Tensorflowë¡œ ì‘ì„±

# multinomial ë¬¸ì œì´ê¸° ë•Œë¬¸ì— labelì„ one-hot encodingì²˜ë¦¬ í•´ì•¼í•´ìš”
# train_t_data, test_t_dataë¥¼ one-hot encodingìœ¼ë¡œ ë³€ê²½í• ê±´ë°
# tensorflowì˜ ê¸°ëŠ¥ì„ ì´ìš©í•´ì„œ ë³€ê²½ => tensorflow nodeë¡œ ìƒì„±
sess = tf.Session()

onehot_train_t_data = sess.run(tf.one_hot(train_t_data, depth=3)) # depthëŠ” class ê°œìˆ˜
onehot_test_t_data = sess.run(tf.one_hot(test_t_data, depth=3))

# tensorflow graphë¥¼ ê·¸ë ¤ë³´ì•„ìš”!
X = tf.placeholder(shape=[None,2], dtype=tf.float32) # ë…ë¦½ë³€ìˆ˜ì˜ ê°œìˆ˜
T = tf.placeholder(shape=[None,3], dtype=tf.float32) # classì˜ ê°œìˆ˜, logisticì˜ ê°œìˆ˜

# Weight & bias
W = tf.Variable(tf.random.normal([2,3]))
b = tf.Variable(tf.random.normal([3]))

# model, Hypothesis
logit = tf.matmul(X,W) + b
H = tf.nn.softmax(logit)

# loss function
loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit,
                                                                 labels=T))

# train
train = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)

# session ì´ˆê¸°í™”
sess.run(tf.global_variables_initializer())

# ë°˜ë³µ
# ë°˜ë³µí•™ìŠµí• ë•Œ ì£¼ì˜í•´ì•¼ í•  ì ì´ ìˆì–´ìš”
# í•™ìŠµë°ì´í„°ì˜ ì‚¬ì´ì¦ˆê°€ ë§¤ìš° í¬ë©´ ë©”ëª¨ë¦¬ì— ë°ì´í„°ë¥¼ í•œë²ˆì— ëª¨ë‘ loadingí•  ìˆ˜ ì—†ì–´ìš”
# memory fault ë‚˜ë©´ì„œ ìˆ˜í–‰ì´ ì •ì§€ëœë‹¤.
# ì–´ë–»ê²Œ í•´ê²°í•´ì•¼ í•˜ë‚˜ìš”? => batch ì²˜ë¦¬ 
# batch ì²˜ë¦¬ ì‹œ ìƒëŒ€ì ìœ¼ë¡œ ì‹œê°„ì´ ë” ì˜¤ë˜ ê±¸ë¦°ë‹¤.

num_of_epoch = 1000    # í•™ìŠµì„ ìœ„í•œ ì „ì²´ epoch ìˆ˜
num_of_batch = 100      # í•œë²ˆì— í•™ìŠµí•  ë°ì´í„° ì–‘

for step in range(num_of_epoch):
    total_batch = int(norm_train_x_data.shape[0] / num_of_batch)
    
    for i in range(total_batch):
        batch_x = norm_train_x_data[i*num_of_batch:(i+1)*num_of_batch]
        batch_y = onehot_train_t_data[i*num_of_batch:(i+1)*num_of_batch]
        _, loss_val = sess.run([train, loss], feed_dict={X: batch_x,
                                                         T: batch_y})

    if step % 100 == 0:
        print(f'loss val : {loss_val}')
    

```

```python
# í•™ìŠµì´ ì¢…ë£Œë˜ì—ˆì–´ìš”!

# ì„±ëŠ¥í‰ê°€(Accuracy)ë¥¼ í•´ì•¼í•´ìš”!
# result = sess.run(H, feed_dict={X:scaler.transform(np.array([[187,81]]))})
# print(result)
# print(np.argmax(result, axis=1)) # ê°€ì¥ í° ê°’ì˜ indexë¥¼ ì•Œë ¤ì¤˜ìš”!
predict = tf.argmax(H,1)
correct = tf.equal(predict, tf.argmax(T,1))
accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))

result = sess.run(accuracy, feed_dict={X:norm_test_x_data,
                                       T:onehot_test_t_data})
print(result)
```


Machine learning

# Machine learning : Multinomial Classification



### 01. Multinomial Classification

ë‹¤ì¤‘ë¶„ë¥˜(Multinomial Classification)ëŠ” classê°€ ì—¬ëŸ¬ê°œì¸ ë¶„ë¥˜ë¥¼ ë§í•œë‹¤.

ë‹¤ìŒì€ ì„±ì ë³„ ë“±ê¸‰ì„ ê·¸ë˜í”„ë¡œ í‘œí˜„í•œ ê·¸ë¦¼ì´ë‹¤.

<img src="../img/image-20220408171147366.png" width="600" height="330">

ì—¬ê¸°ì„œ ë“±ê¸‰ì€ Labelë¡œ A, B, C ì´ 3ê°œì˜ Labelì´ ìˆëŠ”ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

ë°ì´í„°ë¥¼ í† ëŒ€ë¡œ ë…¸ë€ìƒ‰ ë°ì´í„°ì˜ ë“±ê¸‰ì„ ì¶”ì¸¡í•˜ëŠ”ê²ƒì„ Multinomial Classificationìœ¼ë¡œ ë¶„ë¥˜í•œë‹¤.

<img src="../img/image-20220408173648230.png" width="600" height="220">

- ìš°ì„  Labelì„ one-hot encodingìœ¼ë¡œ ì—°ì‚° ê°€ëŠ¥í•œ ìˆ«ìë¡œ ë°”ê¾¸ì–´ì£¼ëŠ” ì‘ì—…ì„ í•´ì•¼í•œë‹¤

- ê° Labelì— ëŒ€í•´ì„œ linear regressionì„ ìˆ˜í–‰í•˜ì—¬ ê·¸ê²ƒì„ ê¸°ì¤€ìœ¼ë¡œ ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ ê° í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥ ì„ êµ¬í•˜ì—¬ ì „ì²´ í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥ ì„ êµ¬í•˜ê²Œ ëœë‹¤.

  

Aì— ëŒ€í•œ ìˆ˜ì‹ì„ ë‚˜íƒ€ë‚´ë©´ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
\text {grade A :}\quad
\begin{pmatrix}
x_{11}&x_{12} \\
x_{21}&x_{22}\\
x_{31}&x_{32}\\
\vdots & \vdots
\end{pmatrix}
\times
\begin{pmatrix}
w_{A1} \\
w_{A2}\\
\end{pmatrix}
+b_A=
\begin{pmatrix}
x_{11}w_{A1}+x_{12}w_{A2}+b_A \\
x_{21}w_{A1}+x_{22}w_{A2}+b_A\\
x_{31}w_{A1}+x_{32}w_{A2}+b_A\\
\vdots
\end{pmatrix}
$$
ì´ë¥¼ ëª¨ë“  classì— ëŒ€í•´ ì ìš©í•˜ë©´ ì‹ì€ ë‹¤ìŒê³¼ ê°™ì•„ì§„ë‹¤.
$$
\text {Matrix :}\quad
\begin{pmatrix}
x_{11}&x_{12} \\
x_{21}&x_{22}\\
x_{31}&x_{32}\\
\vdots & \vdots
\end{pmatrix}
\times
\begin{pmatrix}
w_{A1}&w_{B1}&w_{C1} \\
w_{A2}&w_{B2}&w_{C2}\\
\end{pmatrix}
+
\begin{pmatrix}
b_A&b_B&b_C
\end{pmatrix}
=
\begin{pmatrix}
\ldots&\ldots&\ldots\\
\ldots&\ldots&\ldots\\
\ldots&\ldots&\ldots
\end{pmatrix}
$$

ì´ì „ì— ì´ì§„ë¶„ë¥˜(binary logistic classification)ì—ì„œëŠ” signoid modelì„ ì‚¬ìš©í•˜ì˜€ë‹¤. 
Multinomial Logistic Classificationì—ì„œëŠ” <span style="background-color:#fff5b1;">softmax</span>ë¥¼ modelë¡œ ì‚¬ìš©í•œë‹¤. 

modelì´ ë³€ê²½ë˜ì—ˆìœ¼ë‹ˆ Loss function(Cross Entropy)ë„ ë³€ê²½ëœë‹¤.

### 02. Keras

#### A) MNIST

Multinomial Classification (ë‹¤ì¤‘ë¶„ë¥˜)ì˜ ì˜ˆë¡œ MNISTê°€ ìˆë‹¤.

MNISTëŠ” ì‚¬ëŒì˜ ì†ê¸€ì”¨ë¡œ ì“´ ìˆ«ì ì´ë¯¸ì§€ ë°ì´í„°ì´ë‹¤.

ğŸ‘‰ **ì´ë¯¸ì§€ ë°ì´í„°**

- ì´ë¯¸ì§€ëŠ” pixelë¡œ ì´ë£¨ì§„ pixelì˜ ì§‘í•©ì´ë‹¤.
- ì´ë¯¸ì§€ëŠ” 2ì°¨ì›ì˜ ë°ì´í„°ì— pixelë‹¹ 3ê°œì˜ ìƒ‰ì„ ê°€ì§ˆ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì´ 3ì°¨ì›ì´ë‹¤.
- ì»¬ëŸ¬ì˜ ê²½ìš°ì—ëŠ” 3ì°¨ì›ì´ì§€ë§Œ, í‘ë°±ì˜ ê²½ìš° 2ì°¨ì›ìœ¼ë¡œë„ í‘œí˜„ì´ ê°€ëŠ¥í•˜ë‹¤.

ğŸ‘‰ **ì´ë¯¸ì§€ ë°ì´í„° ì…ë ¥í•˜ê¸°**

- í‘ë°±ì˜ ì´ë¯¸ì§€ë„ ì…ë ¥ë°ì´í„°ë¡œ ì…ë ¥í•˜ê²Œ ë˜ë©´ 2ì°¨ì›ì˜ í˜•íƒœë¡œ ë“¤ì–´ê°€ì§€ê²Œ ëœë‹¤. 
  ì—¬ê¸°ì„œ ì´ë¯¸ì§€ê°€ 1ê°œê°€ ì•„ë‹Œ ì—¬ëŸ¬ê°œë¼ë©´ 3ì°¨ì›ì˜ í˜•íƒœê°€ ëœë‹¤.
- MNISTì˜ ë°ì´í„°ëŠ” ì´ë¯¸ì§€ê°€ 2ì°¨ì›ì´ ì•„ë‹Œ ravel() ì´ ì ìš©ëœ 1ì°¨ì› ë°ì´í„°ì´ë‹¤.
  (ì´ë¯¸ ì´ë¯¸ì§€ì— ëŒ€í•œ ì „ì²˜ë¦¬ëŠ” ë‹¤ ë˜ì–´ìˆë‹¤.)

#### B) Tensorflow

ğŸ‘‰ **Tensorflow 1.5**

- Tensorflow 1.5ë²„ì „ì€ CPU, GPU ë²„ì „ì´ ë”°ë¡œ ì¡´ì¬í•œë‹¤.
- ë°°ìš´ì´ë¡ ì„ ì½”ë“œë¡œ ì´í•´í•˜ê¸°ëŠ” ì¢‹ì§€ë§Œ, ì½”ë“œê°€ ë„ˆë¬´ ì–´ë µë‹¤.
  ê·¸ë˜ì„œ ë‚˜ì˜¨ê²ƒì´ Tensorflow 2.0 â•

ğŸ‘‰ **Tensorflow 2.x**

- Keras ì°½ì‹œìê°€ googleì— ì…ì‚¬í•˜ì—¬ Tensorflowì— Kerasë¥¼ ì‹¬ì–´ì£¼ì—ˆë‹¤.
- Tensorflow2.x ë²„ì „ì€ CPU, GPU ë²„ì „ì´ ë”°ë¡œ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤.
- ì›ë˜ Lazy Excutionì—ì„œ <span style="background-color:#fff5b1;">Eager Execution</span>(ì¦‰ì‹œì‹¤í–‰ëª¨ë“œ) ë¡œ ë³€ê²½ë˜ì–´ session, placeholderê°€ ì—†ì–´ì¡Œë‹¤.
- <span style="background-color:#fff5b1;">Keras</span>ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

ğŸ‘‰ **Tensorflow ì„¤ì¹˜í•˜ê¸°**

- Tensorflow 2.x ë²„ì „ ì„¤ì¹˜

    tensorflowëŠ” ê° í™˜ê²½ì— ë§ëŠ” ë²„ì „ìœ¼ë¡œ ì„¤ì¹˜ëœë‹¤.

    Tensorflow 2.xë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê°€ìƒí™˜ê²½ ìƒˆë¡œ ë§Œë“¤ì–´ ì‚¬ìš©í•  ëª¨ë“ˆì„ ì„¤ì¹˜í•œë‹¤.

    ```python
    > conda create -n machine_TF2 python=3.8 openssl
    ```

    ```python
    > conda activate machine_TF2
    > conda install numpy pandas matplotlib nb_conda tensorflow
    > pip install sklearn
    ```

- Tensorflow ë²„ì „ í™•ì¸í•˜ê¸°

    `print(tf.__version__)`ë¥¼ ì‚¬ìš©í•´ì„œ tensorflowì˜ ë²„ì „ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

    Tensorflow 2.x ë²„ì „ì€ Eager Execution (ì¦‰ì‹œ ì‹¤í–‰ëª¨ë“œ)ë¥¼ ì§€ì›í•œë‹¤.

    - sessionì„ ì‚¬ìš©í•˜ì§€ ì•Šì•„ë„ nodeë¥¼ ì‹¤í–‰ì‹œì¼œì„œ ê°’ì„ ì•Œì•„ë‚¼ ìˆ˜ ìˆë‹¤.
    - ì´ˆê¸°í™” ì½”ë“œë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.
    - placeholderë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.

    ```python
    import tensorflow as tf
    print(tf.__version__) # 2.3.0
    
    W = tf.random.normal([1], dtype=tf.float32)
    
    print(W.numpy()) # [-0.40402368]
    ```

#### C) Keras

##### â—† Multiple Logistic Regression

ë‹¤ìŒì€ Multiple Logistic Regressionì„ í‘œí˜„í•œ ê²ƒì´ë‹¤.

- Data Setì—ì„œ X data ëŠ” Linear Regressionì„ ê±°ì³ ê° ë°ì´í„°ì— ëŒ€í•œ í™•ë¥ ê°’ì„ ê³„ì‚°í•œë‹¤.
- ê³„ì‚°ëœ í™•ë¥ ê°’ì€ sigmoid modelì„ í†µí•´ ê²°ê³¼ê°’ì„ ë§Œë“¤ê²Œ ëœë‹¤.
- ì •ë‹µ Tì™€ ë¹„êµí•˜ë©° loss function(Cross Entropy)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹ì— ë§ëŠ” W, bê°’ì„ ì°¾ëŠ”ë‹¤.

<img src="../img/image-20220408191001995.png" width="700" height="220">

##### â—† Keras binary classification

Kerasì—ëŠ” <span style="background-color:#fff5b1;">Model</span>ì´ë¼ëŠ” ê°œë…ì´ ìˆë‹¤.

Modelì€ layerë¡œ êµ¬ì„±ë˜ì–´ìˆê³ , layerë¥¼ í†µí•´ ì—°ì‚°ì„ ìˆ˜í–‰í•œë‹¤.

- Training Dataê°€ Modelì˜ Input Layerë¥¼ í†µí•´ passing by ëœë‹¤.
  ( ì•„ë¬´ëŸ° ë™ì‘ì„ í•˜ì§€ì•Šê³  x ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ë“¤ì—¬ì˜¤ê¸°ë§Œ í•œë‹¤. )
- Model ì•ˆì— ìˆëŠ” w ê°’ì´ xê°’ê³¼ ê³±í•´ì ¸ Output Layerë¡œ ì´ë™í•œë‹¤.
- Output layerì—ì„œ Logistic Regression ì—°ì‚°ì´ ìˆ˜í–‰ëœë‹¤.
  xW ì— bë¥¼ ë”í•œ í›„ Linear Regression ì—°ì‚°í•˜ê³  sigmoid modelì„ ì‚¬ìš©í•´ì„œ ê²°ê³¼ë¥¼ ë§Œë“ ë‹¤.
- ê²°ê³¼ y ê°’ì´ ë§Œë“¤ì–´ì§€ê³ , ì´ë¥¼ ì •ë‹µê°’ Tì™€ ë¹„êµí•˜ê³  W,b updateë¥¼ ìˆ˜í–‰í•œë‹¤.

<img src="../img/image-20220408191043084.png" width="700" height="400">

##### â—† Keras multinomial classification

multinomialì¼ ê²½ìš° ì´ë ‡ê²Œ í‘œí˜„í•œë‹¤.

- Input Layer ê¹Œì§€ëŠ” ì´ì „ê³¼ ê°™ë‹¤
- Output Layerì—ëŠ” ê° class ë³„ ê³„ì‚°ê²°ê³¼ë¥¼ ë‚´ì¤„ Nodeê°€ ì¡´ì¬í•œë‹¤. 
  Input Layerë¥¼ í†µí•´ ì˜¨ x ë°ì´í„°ëŠ” Modelì— ì¡´ì¬í•˜ëŠ” Wì™€ ì—°ì‚°ë˜ì–´ ê° Nodeë¡œ ì „í•´ì§„ë‹¤.
  ê° Nodeì— ì „ë¶€ ì „ë‹¬ë˜ê¸° ë•Œë¬¸ì— ì´ Layerë¥¼ <span style="background-color:#fff5b1;">FC Layer</span> (Full Connected Layer) ë¼ê³  ë¶€ë¥¸ë‹¤.
- multinomial ë¶„ë¥˜ì—ëŠ” softmax modelì´ ì‚¬ìš©ëœë‹¤.
  bë¥¼ ë”í•˜ê³  linear regressionì„ ë§ˆì¹œ ë’¤ softmax modelì„ ì‚¬ìš©í•˜ì—¬ yê°’ì„ ë§Œë“ ë‹¤.
- ë§Œë“¤ì–´ì§„ ê²°ê³¼ yê°’ì„ ì •ë‹µ tê°’ê³¼ ëŒ€ì¡°í•˜ì—¬ W,bë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤.

<img src="../img/image-20220408193320502.png" width="600" height="400">

##### â—† Keras use

Kerasë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ìœ„ì—ì„œ ê·¸ë¦¼ìœ¼ë¡œ ì„¤ëª…í•œê²ƒë“¤ì„ ì½”ë“œë¡œ êµ¬í˜„í•´ì•¼ í•œë‹¤.

ìš°ì„  Keras Modelì€ `Sequential()`ì„ ì‚¬ìš©í•´ì„œ ìƒì„±ëœë‹¤.

ì´í›„ `add()` ë¥¼ ì‚¬ìš©í•´ì„œ Modelì•ˆì˜ Layerë¥¼ ìƒì„±í•´ì¤€ë‹¤.

`compile()` ë¥¼ ì‚¬ìš©í•˜ì—¬ lossì¢…ë¥˜ì™€ optimizerì¢…ë¥˜ë¥¼ ì„¤ì •í•œë‹¤.

í•™ìŠµì„ í•  ë•ŒëŠ” `fit()`ì„ ì‚¬ìš©í•œë‹¤.

ëª¨ë¸ í‰ê°€ì‹œì—ëŠ” `evaluate()`ë¥¼ ì‚¬ìš©í•˜ê³  ì˜ˆì¸¡ê°’ì„ ë„ì¶œí•  ë•Œì—ëŠ” `predict()`ë¥¼ ì‚¬ìš©í•œë‹¤.

ëª¨ë¸ ì €ì¥ ì‹œì— `save()`ë¥¼ ì‚¬ìš©í•´ì„œ ì €ì¥í•  ìˆ˜ìˆë‹¤.

> ğŸ“Œ ëª¨ë¸ì €ì¥
>
> í•™ìŠµí•œ í›„ ëª¨ë¸ì´ ë©”ëª¨ë¦¬ì— ì €ì¥ë˜ì–´ìˆë‹¤.
> ë©”ëª¨ë¦¬ì— ì €ì¥ëœ ëª¨ë¸ì€ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•˜ë©´ ì—†ì–´ì§„ë‹¤.
>
> kerasì—ì„œëŠ” ëª¨ë¸ì„ ì €ì¥í•˜ê³  ë‹¤ì‹œë¶ˆëŸ¬ì™€ì„œ ì¬í•™ìŠµì´ ê°€ëŠ¥í•˜ë‹¤.
> ì´ë¥¼ í†µí•´ ì‹œê°„ì„ ì ˆì•½í•  ìˆ˜ ìˆê³ , ë‹¤ë¥¸ì‚¬ëŒê³¼ ê³µìœ ë„ ê°€ëŠ¥í•˜ë‹¤.
>
> ëª¨ë¸ì €ì¥ë°©ë²•ì—ëŠ” ë‘ê°€ì§€ê°€ ìˆë‹¤.
>
> - ëª¨ë¸êµ¬ì¡°ì™€ W, bë¥¼ ê°™ì´ì €ì¥ : í¸í•˜ì§€ë§Œ ì‚¬ì´ì¦ˆê°€ í¬ë‹¤
> - W, bë§Œ ì €ì¥ : í¬ê¸°ê°€ ì‘ì§€ë§Œ, ì‚¬ìš©í•˜ë ¤ë©´ ëª¨ë¸ì„ ë§Œë“¤ê³  W, bë¥¼ ë¡œë”©í•´ì•¼í•œë‹¤.



### 03. ì˜ˆì œ

#### ì˜ˆì œ1 

- MNIST ì˜ˆì œ : ë§ˆì§€ë§‰ tensorflow 1.5

```python
# MNIST ì˜ˆì œë¥¼ êµ¬í˜„í•´ë³´ì•„ìš”
# DataëŠ” Kaggle ì—ì„œ ë‹¤ìš´ë¡œë“œ
import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

import warnings
warnings.filterwarnings(action='ignore')


# Raw Data Loading
df = pd.read_csv('./data/mnist/train.csv')
display(df.shape)
```

```python
# ë°ì´í„° ì „ì²˜ë¦¬
# ê²°ì¸¡ì¹˜ë‚˜ ì´ìƒì¹˜ê°€ ì¡´ì¬í•˜ì§€ ì•Šì•„ìš”
# ë‹¨, ì •ê·œí™”ëŠ” í•„ìš”í•´ìš” (scaleì˜ ì°¨ì´ê°€ ë‚˜ì„œ 0,255)
# ì´ë¯¸ì§€ í™•ì¸

figure = plt.figure()
ax_arr = [] # python list

img_data = df.drop('label', axis=1, inplace=False).values

for n in range(10):
    ax_arr.append(figure.add_subplot(2,5,n+1))
    ax_arr[n].imshow(img_data[n].reshape(28,28),
                     cmap='Greys',            # í‘ë°±ì´ë¯¸ì§€ í‘œí˜„
                     interpolation='nearest') # ë³´ê°„ë²• : ì´ë¯¸ì§€ ê¹”ë”í•˜ê²Œ ë³´ì´ê²Œ
plt.tight_layout()
plt.show()
```

```python
# Data Split
train_x_data, test_x_data, train_t_data, test_t_data =\
train_test_split(df.drop('label', axis=1, inplace=False),
                 df['label'],
                 test_size=0.3,
                 random_state=1,
                 stratify=df['label'])

# ì •ê·œí™”
scaler = MinMaxScaler()
scaler.fit(train_x_data)

norm_train_x_data = scaler.transform(train_x_data)
norm_test_x_data = scaler.transform(test_x_data)

```

```python
## Tensorflow Implementation ##
sess = tf.Session()

onehot_train_t_data = sess.run(tf.one_hot(train_t_data, depth=10))
onehot_test_t_data = sess.run(tf.one_hot(test_t_data, depth=10))

# placeholder
X = tf.placeholder(shape=[None,784], dtype=tf.float32)
T = tf.placeholder(shape=[None,10], dtype=tf.float32)

# Weight & bias
W = tf.Variable(tf.random.normal([784,10]))
b = tf.Variable(tf.random.normal([10]))

# Hypothesis, Model
logit = tf.matmul(X,W) + b
H = tf.nn.softmax(logit)

# Loss function
loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit,
                                                                 labels=T))

# train
train = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)

# session, ì´ˆê¸°í™”
sess.run(tf.global_variables_initializer())

# ë°˜ë³µí•™ìŠµ
num_of_epoch = 1000
batch_size = 100

for step in range(num_of_epoch):
    
    total_batch = int(norm_train_x_data.shape[0] / batch_size)

    for i in range(total_batch):
        batch_x = norm_train_x_data[i*batch_size:(i+1)*batch_size]
        batch_y = onehot_train_t_data[i*batch_size:(i+1)*batch_size]
        
        _, loss_val = sess.run([train, loss], feed_dict={X:batch_x,
                                                         T:batch_y})
    if step % 100 == 0:
        print('loss val : {}'.format(loss_val))
```

```python
# accuracy ì¸¡ì •
predict = tf.argmax(H,1)
correct = tf.equal(predict, tf.argmax(T,1))
accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))

accuracy_val = sess.run(accuracy, feed_dict={X: norm_test_x_data,
                                             T: onehot_test_t_data})
print(f'Accuracy : {accuracy_val}')
```

#### ì˜ˆì œ 2 : Keras

```python
# ëŒ€í‘œì ì¸ multinomial ì˜ˆì œì¸ MNISTë¥¼ ì´ìš©í•´ì„œ 
# Tensorflow 2.x ë²„ì „ìœ¼ë¡œ êµ¬í˜„í•´ë³´ì•„ìš”

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense # Flatten(input layer)
                                                    # Dense(output layer)
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
    
# Raw Data Loading

df = pd.read_csv('./data/mnist/train.csv')
display(df.head())
```

```python
# Data Split
train_x_data, test_x_data, train_t_data, test_t_data = \
train_test_split(df.drop('label', axis=1, inplace=False),
                 df['label'],
                 test_size=0.3,
                 random_state=1,
                 stratify=df['label'])

# ì •ê·œí™”
scaler = MinMaxScaler()
scaler.fit(train_x_data)

norm_train_x_data = scaler.transform(train_x_data)
norm_test_x_data = scaler.transform(test_x_data)

```

```python
# Tensorflow 2.x êµ¬í˜„

# model ìƒì„±
model = Sequential()

# layer ì¶”ê°€
# wëŠ” modelì„ ë§Œë“œëŠ” ìˆœê°„ ì¸ìë¥¼ë³´ê³  ì§€ì •ëœë‹¤.
# input layerëŠ” í•˜ëŠ”ì¼ ì—†ì–´ì„œ ì½”ë“œ í•œë²ˆì— ì“¸ìˆ˜ìˆëŠ”ë° ë‚˜ì¤‘ì— í•´ë´ìš”~
model.add(Flatten(input_shape=(norm_train_x_data.shape[1],)))

model.add(Dense(units=10,
                activation='softmax'))

print(model.summary())
```

```python
# model compile
# ì‚¬ìš©í•  loss í•¨ìˆ˜ë¥¼ ì§€ì •, ì‚¬ìš©í•œ optimizer(w,b ì—…ë°ì´íŠ¸ ì•Œê³ ë¦¬ì¦˜)ë¥¼ ì§€ì •
from tensorflow.keras.optimizers import SGD, Adam

# loss
# linear regression : linear :: linear regressionì˜ loss (MSE)
# binary classification : binary_crossentropy
# multinomial classification : categorical_crossentropy (onehot ì²˜ë¦¬ í•„ìš”)
# multinomial classification : sparse_categorical_crossentropy (onehot ì²˜ë¦¬ê°€ í•„ìš”ì—†ìŒ)

model.compile(optimizer=SGD(learning_rate=1e-3),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# í•™ìŠµê²°ê³¼ë¥¼ ë³€ìˆ˜ì— ì €ì¥
history = model.fit(norm_train_x_data,
                    train_t_data,
                    epochs=100,
                    batch_size=100,
                    verbose=1,
                    validation_split=0.2)
```

```python
print(model.evaluate(norm_test_x_data, test_t_data))
#       loss              accuracy
# [0.4791148602962494, 0.8763492107391357]
```

ì˜ˆì œ : Keras ì €ì¥

```python
# ëŒ€í‘œì ì¸ multinomial ì˜ˆì œì¸ MNISTë¥¼ ì´ìš©í•´ì„œ 
# Tensorflow 2.x ë²„ì „ìœ¼ë¡œ êµ¬í˜„í•´ë³´ì•„ìš”

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense # Flatten(input layer)
                                                    # Dense(output layer)
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.optimizers import SGD, Adam
from tensorflow.keras.callbacks import ModelCheckpoint
    
# Raw Data Loading

df = pd.read_csv('./data/mnist/train.csv')
display(df.head())
```

```python
# Data Split
# ê¸°ì¡´ì—ëŠ” test_x_data, test_t_data ì´ ë‘ë°ì´í„°ë¥¼ validation ìš©ë„ë¡œ ì‚¬ìš©
# ì´ì œëŠ” test_x_data, test_t_data ì´ ë‘ ë°ì´í„°ë¥¼ test ìš©ë„ë¡œ ì‚¬ìš©í• ê±°ì—ìš”
# ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ë¥¼ ìœ„í•´ ë”± 1ë²ˆë§Œ ì‚¬ìš©í• ê±°ì—ìš”
# ê·¸ëŸ¼ validationì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?
# kerasëŠ” í•™ìŠµí•  ë•Œ train dataë¥¼ ì¼ì •ë¶€ë¶„ ë‚˜ëˆ„ì–´ì„œ ìì²´ validationì´ ê°€ëŠ¥
# keras ê¸°ëŠ¥ì„ ì´ìš©í•˜ì—¬ validation ì²˜ë¦¬

train_x_data, test_x_data, train_t_data, test_t_data = \
train_test_split(df.drop('label', axis=1, inplace=False),
                 df['label'],
                 test_size=0.3,
                 random_state=1,
                 stratify=df['label'])

# ì •ê·œí™”
scaler = MinMaxScaler()
scaler.fit(train_x_data)

norm_train_x_data = scaler.transform(train_x_data)
norm_test_x_data = scaler.transform(test_x_data)

# loss ì§€ì •í•  ë•Œ sparse_categorical_crossentropyë¡œ lossí•¨ìˆ˜ë¥¼ ì§€ì •í•  ì˜ˆì •ì´ê¸° ë•Œë¬¸ì—
# target(label)ì— ëŒ€í•œ onehot encoding ì²˜ë¦¬ê°€ í•„ìš”ì—†ë‹¤.
```

```python
# Tensorflow 2.x êµ¬í˜„

# model ìƒì„±
model = Sequential()

# layer ì¶”ê°€
model.add(Flatten(input_shape=(norm_train_x_data.shape[1],)))

model.add(Dense(units=10,
                activation='softmax'))

# model compile
# ì‚¬ìš©í•  loss í•¨ìˆ˜ë¥¼ ì§€ì •, ì‚¬ìš©í•œ optimizer(w,b ì—…ë°ì´íŠ¸ ì•Œê³ ë¦¬ì¦˜)ë¥¼ ì§€ì •

# loss
model.compile(optimizer=SGD(learning_rate=1e-3),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

#----------------------check point------------------------
# modelì„ ì €ì¥í•˜ë ¤ê³  í•´ìš”. modelêµ¬ì¡°ëŠ” ë¹¼ê³  weight, b ë§Œ ì €ì¥
checkpoint_path = './training_ckpt/cp.ckpt'
# checkpoint_dir = os.path.dirname(checkpoint_path) # ì‹¤ì œ ê²½ë¡œë¡œ ë§Œë“¤ì–´ìš”
cp_callback = ModelCheckpoint(filepath=checkpoint_path,
                              save_weights_only=True,
                              verbose=1)

#---------------------------------------------------------

# í•™ìŠµê²°ê³¼ë¥¼ ë³€ìˆ˜ì— ì €ì¥
history = model.fit(norm_train_x_data,
                    train_t_data,
                    epochs=100,
                    batch_size=100,
                    verbose=1,  # 0ìœ¼ë¡œ ì„¤ì • ì‹œ ì¶œë ¥ì´ ë‚˜ì˜¤ì§€ ì•ŠëŠ”ë‹¤.
                    validation_split=0.2,
                    callbacks=[cp_callback]) # checkpoint ì— ì €ì¥
# í‰ê°€ì§„í–‰

print(model.evaluate(norm_test_x_data, test_t_data))
#       loss              accuracy
# [0.47676777839660645, 0.8751587271690369]
```

```python
%reset
# ë¶ˆëŸ¬ì„œ ë‹¤ì‹œ ì‚¬ìš©í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼í•˜ë‚˜ìš”?

# í™•ì¸í•˜ê¸° ìœ„í•´...
# í•™ìŠµí•˜ì§€ ì•Šì€ ìƒíƒœë¡œ evaluationì„ ì§„í–‰í•˜ë©´ ë‹¹ì—°íˆ í‰ê°€ê²°ê³¼ê°€ ì¢‹ì§€ì•Šê² ì£ ?

# ëŒ€í‘œì ì¸ multinomial ì˜ˆì œì¸ MNISTë¥¼ ì´ìš©í•´ì„œ 
# Tensorflow 2.x ë²„ì „ìœ¼ë¡œ êµ¬í˜„í•´ë³´ì•„ìš”

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense # Flatten(input layer)
                                                    # Dense(output layer)
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.optimizers import SGD, Adam
from tensorflow.keras.callbacks import ModelCheckpoint
    
# Raw Data Loading

df = pd.read_csv('./data/mnist/train.csv')
display(df.head())

# Data Split
train_x_data, test_x_data, train_t_data, test_t_data = \
train_test_split(df.drop('label', axis=1, inplace=False),
                 df['label'],
                 test_size=0.3,
                 random_state=1,
                 stratify=df['label'])

# ì •ê·œí™”
scaler = MinMaxScaler()
scaler.fit(train_x_data)

norm_train_x_data = scaler.transform(train_x_data)
norm_test_x_data = scaler.transform(test_x_data)

# Tensorflow 2.x êµ¬í˜„

# model ìƒì„±
model = Sequential()

# layer ì¶”ê°€
model.add(Flatten(input_shape=(norm_train_x_data.shape[1],)))

model.add(Dense(units=10,
                activation='softmax'))

# model compile
model.compile(optimizer=SGD(learning_rate=1e-3),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# ì›ë˜ í•™ìŠµì„ ì§„í–‰í•˜ì§€ ì•Šê³  ìµœì¢…í‰ê°€ ì§„í–‰
# í‰ê°€ì§„í–‰
print(model.evaluate(norm_test_x_data, test_t_data))
#                loss              accuracy
# í•™ìŠµ x [2.4072484970092773, 0.11619047820568085]
```

```python
# ì´ë²ˆì—”ã„´ checkpoint íŒŒì¼ì— ìˆëŠ” weightë¥¼ load í•œ í›„
# evaluation ì‹œì¼œë³´ì•„ìš”

checkpoint_path = './training_ckpt/cp.ckpt'
model.load_weights(checkpoint_path)
print(model.evaluate(norm_test_x_data, test_t_data))
# [0.47676777839660645, 0.8751587271690369]
```


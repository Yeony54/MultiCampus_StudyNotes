Machine learning

# Machine learning : Classification



conda install mglearn : ì‚°í¬ë„ ê·¸ë˜í”„ë¡œ ê³¼ì •ì„ í™•ì¸í•˜ê¸° ì‰¬ì›€



### 01. Logistic Regression

- Classificationì—ëŠ” ì´í•­ë¶„ë¥˜(binary classification), ë‹¤í•­ë¶„ë¥˜(Multinomial classification)ê°€ ìˆë‹¤.

- Logistic Regressionì€ Linear Regressionì„ í™•ì¥í•œ ë¶„ë¥˜ì´ë‹¤. 

- Logistic Regressionì€ Deep Learningìœ¼ë¡œ ì—°ê²°ë˜ëŠ” ê¸°ë³¸ component ì´ë‹¤.

#### A) ê°œìš”

> ğŸ“Œ ì´ˆê¸° ì¸ê³µì§€ëŠ¥ ì•Œê³ ë¦¬ì¦˜
>
> **Perceptron** ì€ ì´ˆê¸° ì¸ê³µì§€ëŠ¥ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ, ê²°ê³¼ê°’ zë¥¼ step function(ê³„ë‹¨í•¨ìˆ˜)ë¥¼ ì‚¬ìš©í•˜ì—¬ 0ë³´ë‹¤ í¬ë©´ 1, 0ë³´ë‹¤ ì‘ìœ¼ë©´ -1ë¡œ ë‘ê°œì˜ ê°’ìœ¼ë¡œ ë¶„ë¥˜í–ˆë˜ ì•Œê³ ë¦¬ì¦˜ì´ë‹¤.
>
> ê·¸ëƒ¥ ì´ˆê¸° ì•Œê³ ë¦¬ì¦˜ì´ë¼ëŠ”ê±°ì§€, ì´ê²Œ í™•ì¥ë˜ì–´ì„œ ì§€ê¸ˆì˜ ì•Œê³ ë¦¬ì¦˜ì´ ë˜ì—ˆë‹¤ë˜ê±°ëŠ” ì•„ë‹ˆë‹¤.

![image-20220405222109002](../img/image-20220405222109002.png)

Logistic Regressionì€ Linear Regressionì— í™œì„±í™” í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ 0ê³¼ 1ì‚¬ì´ì˜ í™•ë¥ ê°’ aë¥¼ ê²°ê³¼ë¡œ ë§Œë“ ë‹¤.

ê·¸ë¦¬ê³  ì´ ê²°ê³¼ê°’ì„ ì„ê³„í•¨ìˆ˜ë¥¼ í†µí•´ ë¶„ë¥˜í•˜ì—¬ ìµœì¢… ê²°ê³¼ê°’ì„ ë§Œë“¤ê²Œ ëœë‹¤.

> ğŸ‘‰ Logistic Regression ì˜ˆì‹œ
>
> ê³µì¥ìƒì‚° ì œí’ˆ ì •ìƒ/ë¶ˆëŸ‰ íŒë… , CT ì‚¬ì§„ìœ¼ë¡œ ì•”/ì •ìƒ íŒë…, 
> ì£¼ì‹ ì½”ì¸ ë“±ì´ ë‹¤ìŒë‚  ì˜¤ë¥¼ì§€/ë‚´ë¦´ì§€, ì‹ ìš©ì¹´ë“œì˜ ìƒíƒœê°€ ì •ìƒì‚¬ìš©ìƒíƒœ/ë„ë‚œì¹´ë“œ íŒë…

<img src="../img/image-20220405222742705.png" width="700" height="400">

ë‹¤ìŒì€ mglearnì—ì„œ ì œê³µí•˜ëŠ” datasetì„ scatterê·¸ë˜í”„ë¡œ í‘œí˜„í•˜ê³ , Linear Regression ì„ ì„ ê·¸ë¦°ê²ƒì´ë‹¤.

1. Linear Regressionì„ ì´ìš©í•´ì„œ Training Data Setì˜ íŠ¹ì„±ê³¼ ë¶„í¬ë¥¼ ì˜ í‘œí˜„í•˜ëŠ” ì§ì„ ì„ ì°¾ëŠ”ë‹¤. 
2. ì´ ê²½ê³„ì„ ì„ ê¸°ì¤€ìœ¼ë¡œ 0ê³¼ 1ë¡œ ë¶„ë¥˜ëœë‹¤.

ì´ ë°©ë²•ì€ ì •í™•ë„ê°€ ìƒë‹¹íˆ ë†’ì•„ Deep Learningì˜ ê¸°ë³¸ componentë¡œ ì‚¬ìš©ëœë‹¤.

> â“ ê·¸ëŸ¼ ì´ì§„ë¶„ë¥˜ë¬¸ì œë¥¼ Logistic Regressionì´ ì•„ë‹Œ Linear Regressionìœ¼ë¡œ ë¶„ë¥˜í•˜ë©´ ë˜ì§€ ì•Šì„ê¹Œ?
>
> ì´ìƒì¹˜ ë“±ì˜ ë¬¸ì œì—ì„œëŠ” Linear Regressionìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ì—†ëŠ” ë¬¸ì œê°€ ìˆë‹¤.
>
> ì˜ˆë¥¼ë“¤ì–´ ê³µë¶€ì‹œê°„ì— ë”°ë¥¸ í•©ê²©ë¥ ì„ ë”°ì¡Œì„ ë•Œ 100ì‹œê°„ì„ ê³µë¶€í•œ ì´ìƒì¹˜ê°€ ì¡´ì¬í•œë‹¤ë©´, ì¶©ë¶„íˆ í•©ê²©í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ë¥¼ ì˜ëª» ë¶„ë¥˜ í•  ìˆ˜ ìˆë‹¤.

#### B) Cross Entropy

ì•ì„œ Logistic Regressionì€ Linear Regressionì— í™œì„±í™”í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ ê²°ê³¼ë¥¼ ì¶”ì¶œí•œë‹¤ê³  í•˜ì˜€ë‹¤.

Logistic Regressionì—ì„œëŠ” ì´ í™œì„±í™” í•¨ìˆ˜ë¡œ <span style="color:red">Sigmoid</span> í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤.
$$
\text Sigmoid = {1\over 1+e^{-x}}
$$
<img src="../img/image-20220405224036593.png" width="300" height="200">

Sigmoidì˜ ê·¸ë˜í”„ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê·¸ë˜í”„ë¡œ ê·¸ë ¤ì§€ë©°, ì´í•¨ìˆ˜ë¥¼ ì ìš©í•˜ë©´ 0ê³¼ 1ì‚¬ì´ì˜ ì‹¤ìˆ˜ë¡œ ë³€í™˜ëœë‹¤.

<span style="color:red">Linear Regression ê³µì‹ì— Sigmoidë¥¼ ì ìš©</span>í•˜ê³ , MSEë¥¼ ì‚¬ìš©í•œ loss functionì„ ë§Œë“¤ë©´ ë‹¤ìŒê³¼ ê°™ì•„ì§„ë‹¤.
$$
\text {Linear Regression Model :} \quad \hat y=Wx+b \qquad\qquad\qquad\qquad
\\
\text {Linear Regression loss func :} \quad E(W,b)={1\over n}\sum_{i=1}^n[t_{i}-(wx_{i}+b)]^2
\\
\\
\text {Logistic Regression Model :} \quad \hat y = {1\over 1+e^{-(Wx+b)}} \qquad\qquad\qquad
\\
\qquad\qquad\qquad\quad\text {New loss func :} \quad E(W,b) = {1\over n}\sum_{i=1}^n\Big[t_i -({1\over e^{(Wx_i+b)}}) \Big]^2
$$

<img src="../img/image-20220405225751373.png" width="300" height="200">

ê·¸ë˜í”„ë¥¼ ë³´ë©´ Convex(ë³¼ë¡)í•œ í˜•íƒœê°€ ì•„ë‹Œê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. 

ì§€ìˆ˜í•¨ìˆ˜(e<sup>-x</sup>)ì˜ íŠ¹ì„±ìœ¼ë¡œ ëª¨ì–‘ì´ êµ¬ë¶ˆêµ¬ë¶ˆí•˜ë‹¤.

ì´ëŒ€ë¡œëŠ” MSEë¥¼ ì´ìš©í•œ loss functionì„ ì‚¬ìš©í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— ì‹ì— <span style="color:red">Log</span>ë¥¼ ì·¨í•´ì£¼ëŠ” ë°©ì‹ì„ ì„ íƒí•œë‹¤.

$$
\text {Logistic Regression loss function :} \quad
=-\sum_{i=1}^n \Big\{t_i \log y_i + (1-t_i) \log  (1-y_i) \Big\}
$$

> log ê³µì‹ì„ ì‚¬ìš©í•˜ë ¤ë©´ c=log<sub>b</sub>a ê³µì‹ì—ì„œ aâ‰ 0ì˜ ì¡°ê±´ì´ í•„ìš”í•˜ë‹¤.
>
> ê·¸ë˜ì„œ í˜¹ì‹œë‚˜ 0ì´ ë“¤ì–´ì˜¬ ê²ƒì„ ëŒ€ë¹„í•´ í”„ë¡œê·¸ë¨ì ìœ¼ë¡œ ê³„ì‚°í•´ì£¼ê¸° ìœ„í•´ ì•½ê°„ì˜ delta ê°’ì„ ì£¼ê²Œ ëœë‹¤.

ì´ ì‹ì„ <span style="background-color:#fff5b1;">Cross Entropy</span>  ë˜ëŠ” log lossë¼ê³  ë¶€ë¥¸ë‹¤.

### 02. Logistic Regression êµ¬í˜„

#### A) Python

```python
# Python êµ¬í˜„

import numpy as np

########## ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì— ëŒ€í•œ ìˆ˜ì¹˜ë¯¸ë¶„ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ ########

def numerical_derivative(f, x):
    
    delta_x = 1e-4
    derivative_x = np.zeros_like(x) 
    it = np.nditer(x, flags=['multi_index'])
    
    while not it.finished:
        
        idx = it.multi_index
        tmp = x[idx]
    
        x[idx] = tmp + delta_x
        fx_plus_delta = f(x)
        
        x[idx] = tmp -delta_x
        fx_minus_delta = f(x)
        
        derivative_x[idx] = (fx_plus_delta - fx_minus_delta) / (2*delta_x)
        
        x[idx] = tmp
        it.iternext()
        
    return derivative_x

################ ìˆ˜ì¹˜ë¯¸ë¶„ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ ë ###########


# Training Data Set
x_data = np.array([2,4,6,8,10,12,14,16,18,20]).reshape(-1,1)
t_data = np.array([0,0,0,0,0,0,1,1,1,1]).reshape(-1,1)

# Weight, bias ì •ì˜
W = np.random.rand(1,1)
b = np.random.rand(1)

# Logiscit Regression model, predict model, hypothesis
def predict(x):
    z = np.dot(x,W)+b               # linear regression model
    y = 1 / (1 + np.exp(-1 * z))   # logistic regression model
    result = 0
    # ê³„ì‚°ë˜ëŠ” y ê°’ì€ ê²°êµ­ 0ê³¼ 1ì‚¬ì´ì˜ í™•ë¥ ê°’
    if y >= 0.5:
        result = 1
    else : 
        result = 0
    return y,result

# Cross Entropy(log loss)
def loss_func(input_data):  # [W,b]
    
    input_W = input_data[:-1].reshape(-1,1)
    input_b = input_data[-1]
    
    z = np.dot(x_data, input_W) + input_b
    y = 1 / (1 + np.exp(-1 *z))
    
    delta = 1e-7
    
    # cross entropy
    return -1 * np.sum(t_data*np.log(y+delta)+(1-t_data)*np.log(1-y+delta))

# learning_rate
learning_rate = 1e-4

# ë°˜ë³µí•™ìŠµ
for step in range(300000):
    input_param = np.concatenate((W.ravel(), b.ravel()), axis=0) # [W b]
    derivate_result = learning_rate * numerical_derivative(loss_func, input_param)
    
    W = W - derivate_result[:-1].reshape(-1,1)
    b = b - derivate_result[-1]
    
    if step % 30000 == 0 :
        input_param = np.concatenate((W.ravel(), b.ravel()), axis=0)
        print('W:{}, b:{}, loss:{}'.format(W, b, loss_func(input_param)))  
```

```python
# prediction
study_hour = np.array([[13]])
y_prob, result = predict(study_hour)
print('í•©ê²©í™•ë¥ :{}, í•©ê²©ì—¬ë¶€:{}'.format(y_prob, result))
# í•©ê²©í™•ë¥ :[[0.54438019]], í•©ê²©ì—¬ë¶€:1
```

#### B) sklearn

```python
# sklearnìœ¼ë¡œ êµ¬í˜„
from sklearn import linear_model
ã… 
x_data = np.array([2, 4, 6, 8,10,12,14,16,18,20]).reshape(-1,1)
t_data = np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1])

model = linear_model.LogisticRegression()

model.fit(x_data, t_data)

study_hour = np.array([[13]])

result = model.predict(study_hour) # ìµœì¢…ê²°ê³¼ë§Œ ì•Œë ¤ì¤€ë‹¤.
result_prob = model.predict_proba(study_hour)

print('í•©ê²©í™•ë¥ :{}, í•©ê²©ì—¬ë¶€:{}'.format(result_prob, result))
# í•©ê²©í™•ë¥ :[[0.50009391 0.49990609]], í•©ê²©ì—¬ë¶€:[0]
```

#### C) Tensorflow

```python
# Tensorflow êµ¬í˜„

import tensorflow as tf

x_data = np.array([2, 4, 6, 8,10,12,14,16,18,20]).reshape(-1,1)
t_data = np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1]).reshape(-1,1)

# placeholdeer
X = tf.placeholder(shape=[None,1], dtype=tf.float32)
T = tf.placeholder(shape=[None,1], dtype=tf.float32)

# Weight & bias
W = tf.Variable(tf.random.normal([1,1]))
b = tf.Variable(tf.random.normal([1]))

# Model(Hypothesis)
logit = tf.matmul(X,W) + b # linear regression model
H = tf.sigmoid(logit)      # logistic regression model

# loss function
loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,
                                                              labels=T))

# train
train = tf.train.GradientDescentOptimizer(learning_rate=1e-3).minimize(loss)

# Session & ì´ˆê¸°í™”
sess = tf.Session()
sess.run(tf.global_variables_initializer())

# ë°˜ë³µí•™ìŠµ
for step in range(30000):
    _, W_val, b_val, loss_val = sess.run([train, W, b, loss],
                                         feed_dict={X: x_data,
                                                    T: t_data})
    if step % 3000 ==0:
        print('W:{}, b:{}, loss:{}'.format(W_val, b_val, loss_val))
```

```python
# prediction
study_hour = np.array([[13]]) # 12ì‹œê°„ì€ ë¶ˆí•©ê²©ì´ì—ˆê³ , 14ì‹œê°„ì€ í•©ê²©ì´ì—ˆì–´ìš”!
result = sess.run(H, feed_dict={X: study_hour})
print('í•©ê²©í™•ë¥ :{}'.format(result))
# í•©ê²©í™•ë¥ :[[0.58296657]]
```


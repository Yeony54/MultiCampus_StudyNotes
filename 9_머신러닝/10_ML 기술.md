Machine learning

# Machine learning



### 01. Log

##### A) ë¡œê·¸ í‘œí˜„ë°©ì‹

LogëŠ” ì§€ìˆ˜ë¥¼ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ í‘œí˜„í•œë‹¤.

> 3<sup>x</sup> = 81, x = 4 (ì§€ìˆ˜ë°©ì •ì‹)
>
> log<sub>3</sub> 81 = x, x = 4 (ë¡œê·¸ë°©ì •ì‹)

ì—¬ê¸°ì„œ `3`ì€ ë°‘, `81`ì€ ì§„ìˆ˜ `x`ëŠ” ì§€ìˆ˜ë¼ê³  í•œë‹¤.

ì§€ìˆ˜ë°©ì •ì‹ì€ ì§„ìˆ˜ê°€ ë¶„ë¦¬ëœí˜•íƒœì´ê³ , ë¡œê·¸ë°©ì •ì‹ì€ ì§€ìˆ˜ê°€ ë¶„ë¦¬ëœ í˜•íƒœì´ë‹¤.

##### B) ë¡œê·¸ ì¢…ë¥˜

- ìƒìš©ë¡œê·¸ (common logarithm) : ì‹­ì§„ë¡œê·¸

  ë°‘ì´ 10ì¸ ë¡œê·¸ 

  > log<sub>10</sub> (x) = log(x)

- ìì—°ë¡œê·¸ (natural logarithm) : ìì—°ë¡œê·¸

  ë°‘ì´ ìì—°ìƒìˆ˜ e (=2.718...) ì¸ ë¡œê·¸

  > log<sub>e</sub> (x) = ln (x)

##### C) ë¡œê·¸ë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ 

- ìˆ˜í•™

  ì§€ìˆ˜ë°©ì •ì‹ì„ ì‰½ê²Œ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ ë¡œê·¸ë°©ì •ì‹ì„ ì´ìš©í•œë‹¤.

- ë¨¸ì‹ ëŸ¬ë‹

  ì •ê·œì„±ì„ ë†’ì´ê¸° ìœ„í•´ì„œ ì‚¬ìš©í•œë‹¤.

  > 1000 â˜ log<sub>10</sub> 1000 =  log<sub>10</sub> 10<sup>3</sup> = 3
  >
  > 100000000 â˜ log<sub>10</sub> 10<sup>8</sup> = 8

### 02. derivative (ë¯¸ë¶„)

##### A) ë¯¸ë¶„ ê°œìš”

- ë¯¸ë¶„ì˜ ì •ì˜

  \- ì–´ë–¤ í•¨ìˆ˜ì˜ ì •ì˜ì—­ ì† ê° ì ì—ì„œ ë…ë¦½ë³€ìˆ˜ ê°’ì˜ ë³€í™”ëŸ‰ê³¼ í•¨ìˆ˜ê°’ìœ¼ ã…£ë³€í™”ëŸ‰ì˜ ë¹„ìœ¨ì˜ ê·¹í•œ, ê·¹í•œì˜ ì§‘í•©ì„ ì¹˜ì—­ìœ¼ë¡œ ê°€ì§€ëŠ” ìƒˆë¡œìš´ í•¨ìˆ˜

  \- í•¨ìˆ˜ì— ëŒ€í•´ì„œ íŠ¹ì • ìˆœê°„ì˜ ë³€í™”ëŸ‰

- ë¯¸ë¶„ì˜ ì¢…ë¥˜

  - í•´ì„ë¯¸ë¶„ : ì¢…ì´ì™€ íœì„ ì´ìš©í•´ ë…¼ë¦¬ì ì¸ ì „ê°œë¡œ ìˆ˜í–‰í•˜ëŠ” ë¯¸ë¶„
  - ìˆ˜ì¹˜ë¯¸ë¶„ : í•´ì„ë¯¸ë¶„ìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ì—†ì„ ë•Œ ìˆ˜ì¹˜ê°’ì„ ì´ìš©í•´ ë¯¸ë¶„ì˜ ê·¼ì‚¬ê°’ì„ ì•Œì•„ë‚´ëŠ” ë°©ë²•

##### B)  ë¯¸ë¶„

ë¯¸ë¶„ì€ í•¨ìˆ˜ì— ëŒ€í•´ì„œ íŠ¹ì • ìˆœê°„ì˜ ë³€í™”ëŸ‰ì„ ë‚˜íƒ€ë‚¸ë‹¤.

ì¦‰ xì—ì„œì˜ ì‘ì€ ë³€í™”ê°€ f(x)ë¥¼ ì–¼ë§ˆë‚˜ ë³€í™”ì‹œí‚¤ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.

ë¯¸ë¶„ì„ ìˆ˜ì‹ìœ¼ë¡œ ì•Œì•„ë³´ì

<table>
      <tr>
        <th style="text-align:center;"><img src="../img/image-20220325174802786.png" width="500" height="350"></th>
        <td >
            ì—¬ê¸°ì„œ ì§ì„ ì˜ ê¸°ìš¸ê¸°ê°€ ë¹„ìœ¨ì´ë‹¤.<br>
            ë§Œì•½ â–³x â‡’ 0 ìœ¼ë¡œ 0ì— ê°€ê¹Œì›Œì§€ê²Œ ë˜ë©´<br>
            xìœ„ì¹˜ì—ì„œ f(x)ì˜ ì ‘ì„ ì´ ëœë‹¤.<br>
            ì´ ê¸°ìš¸ê¸°ê°€ ë¯¸ë¶„ê°’(ë³€í™”ëŸ‰)ì´ë‹¤.
         </td>  
      </tr>
</table>

$$
\lim_{x \to 0} {f(x+\Delta x) - f(x)\over \Delta x}
$$

##### C) ìˆ˜ì¹˜ë¯¸ë¶„

ìˆ˜ì¹˜ë¯¸ë¶„ì˜ ì¢…ë¥˜ë¡œëŠ” **ì „í–¥ë¯¸ë¶„, í›„í–¥ë¯¸ë¶„, ì¤‘ì•™ì°¨ë¶„**ì´ ìˆìœ¼ë©° ê·¸ ì¤‘ <span style="color:red">ì¤‘ì•™ì°¨ë¶„</span>ì´ ê°€ì¥ ê°’ì´ ì •í™•í•˜ê¸° ë•Œë¬¸ì— ë§ì´ ì‚¬ìš©ëœë‹¤.

ë¯¸ë¶„ê³¼ ì˜ë¯¸ëŠ” ë˜‘ê°™ìœ¼ë©°, xì˜ ì¢Œìš°ë¡œ â–³xë§Œí¼ ë–¨ì–´ì§„ ê³³ìœ¼ë¡œ ë¶€í„° ê³„ì‚°í•œ ê°’ì„ ì‚¬ìš©í•œë‹¤.

<table>
      <tr>
        <th style="text-align:center;"><img src="../img/image-20220325174827447.png" width="480" height="350"></th>
        <td >
            â–³x â‡’ 0 ìœ¼ë¡œ 0ì— ê°€ê¹Œì›Œì§€ê²Œ ë˜ë©´<br>
            xìœ„ì¹˜ì—ì„œ f(x)ì˜ ì ‘ì„ ì´ ëœë‹¤.<br>
            ì´ ê¸°ìš¸ê¸°ê°€ ë¯¸ë¶„ê°’(ë³€í™”ëŸ‰)ì´ë‹¤.
         </td>  
      </tr>
</table>

$$
\lim_{x \to 0} {f(x+\Delta x) - f(x-\Delta x)\over 2\Delta x}
$$

> ğŸ“Œ í”„ë¡œê·¸ë¨ì—ì„œ â–³x
>
> í”„ë¡œê·¸ë¨ìœ¼ë¡œ ë¯¸ë¶„ì„ êµ¬í˜„ í•  ë•Œì— â–³xëŠ” ì–´ë–»ê²Œ í•´ì•¼í• ê¹Œ?
>
>  â–³xëŠ” 0ì— ê°€ê¹Œìš´ ì•„ì£¼ ì‘ì€ ê°’ì´ë¯€ë¡œ 0.000...1 ì˜ê°’ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.
>
> í•˜ì§€ë§Œ í”„ë¡œê·¸ë˜ë°ì ìœ¼ë¡œ 10<sup>-8</sup>ì´í•˜ì˜ ê°’ì„ ì‚¬ìš©í•œë‹¤ë©´ ë¬¸ì œê°€ ë°œìƒí•˜ê¸° ë•Œë¬¸ì—
>
> ì´ë²ˆ í”„ë¡œê·¸ë˜ë° ì‹¤ìŠµì—ì„œëŠ” â–³xê°’ì„ 10<sup>-4</sup>ë¡œ ì„¤ì •í•˜ì˜€ë‹¤.



##### D) ê¸°ë³¸ ë¯¸ë¶„ ê³µì‹

- f(x) = constant

  f'(x) = 0

- f(x) = ax<sup>n</sup>

  f'(x) = nÂ·ax<sup>n-1</sup>

- f(x) = e<sup>x</sup>

  f'(x) = e<sup>x</sup>

- f(x) = e<sup>-x</sup>

  f'(x) = -e<sup>-x</sup>

- f(x) = ln(x) = log<sub>e</sub>(x)

  f'(x) = 1/x

##### E) í¸ë¯¸ë¶„ (partial deviation)

ë…ë¦½ë³€ìˆ˜ê°€ 2ê°œ ì´ìƒì¸ ë‹¤ë³€ìˆ˜í•¨ìˆ˜ì—ì„œ ë¯¸ë¶„í•˜ê³ ì í•˜ëŠ” ë³€ìˆ˜ í•˜ë‚˜ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë³€ìˆ˜ë¥¼ ìƒìˆ˜ë¡œ ì·¨ê¸‰í•´ì„œ í•´ë‹¹ ë³€ìˆ˜ë¥¼ ë¯¸ë¶„í•˜ëŠ” ë°©ë²•
$$
f(x,y) = 2x + 3xy + y^3
\\\\
{\delta f(x,y)  \over \delta x} = 2 + 3y
\\
{\delta f(x,y)  \over \delta y} = 3x + 3y^2
$$

##### F) ë¯¸ë¶„ì˜ Chain Rule (ì—°ì‡„ë²•ì¹™)

ì—¬ëŸ¬ í•¨ìˆ˜ë¡œ êµ¬ì„±ëœ í•©ì„±í•¨ìˆ˜ë¥¼ ë¯¸ë¶„í•  ë•Œ ì‚¬ìš©
$$
f(x) = 3^{3x^2}
\\
\text{function1 : } e^t
\\
\text{function2 : } t = 3x^2
\\\\
{\delta f(x)\over \delta x} = {\delta f\over \delta t}\times {\delta t\over \delta x}
\\\\
f'(x) = e^t \times 6x = 6xe^{3x^2}
$$


### 03. ë¯¸ë¶„ êµ¬í˜„

##### A) first-classes function

í•œêµ­ì–´ë¡œëŠ” ì¼ê¸‰í•¨ìˆ˜ë¼ê³  í•˜ë©° javascript, pythonì—ì„œ ì´ë¥¼ ì§€ì›í•œë‹¤.

ì¼ê¸‰í•¨ìˆ˜ëŠ” í•¨ìˆ˜ê°€ ë³€ìˆ˜ì— ì €ì¥ë  ìˆ˜ ìˆì–´ í•¨ìˆ˜ì´ë¦„ì„ ë³€ìˆ˜ì²˜ëŸ¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

```python
def my_func3(x):
    x(20)
    
def my_func4(x):
    print(x+30)

my_func3(my_func4) # 50
```

##### B) ë‹¨ë³€ìˆ˜ ë¯¸ë¶„ í•¨ìˆ˜

ë‹¤ìŒ ì¤‘ì•™ì°¨ë¶„ ê³µì‹ì„ í”„ë¡œê·¸ë¨ìœ¼ë¡œ êµ¬í˜„í•´ë³´ì.
$$
\lim_{x \to 0} {f(x+\Delta x) - f(x-\Delta x)\over 2\Delta x}
$$

```python
# ë‹¨ë³€ìˆ˜ í•¨ìˆ˜
# f(x) = x^2
# f'(x) = 2x
# f'(5) = 10

# f(x)
def my_func(x):
    return x**2

# ë¯¸ë¶„ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜
def numerical_derivative(f, x):
    
    delta_x = 1e-4 # ê·¹í•œì— í•´ë‹¹í•˜ëŠ” ê°’. ë„ˆë¬´ ì‘ì€ ê°’ì„ ì‚¬ìš©í•˜ë©´ ì‹¤ìˆ˜ê³„ì‚° ì˜¤ë¥˜ê°€ ë°œìƒí•´ìš”.
                   # 1e-4 ì •ë„ì˜ ê°’ì„ ì´ìš©í•˜ë©´ ì ë‹¹í•œ ìˆ˜ì¹˜ë¯¸ë¶„ ê°’ì„ êµ¬í•  ìˆ˜ ìˆì–´ìš”.
    return (f(x + delta_x) - f(x-delta_x)) / (2*delta_x)
    
    
# f'(5)
result = numerical_derivative(my_func, 5)
print(result)    # 9.999999999976694 ì•½ê°„ ì°¨ì´ê°€ ìˆëŠ” ê·¼ì‚¬ê°’ì„ ì–»ì–´ë‚¼ ìˆ˜ ìˆë‹¤.
```

##### C) ë‹¤ë³€ìˆ˜ ë¯¸ë¶„í•¨ìˆ˜

ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ëŠ” 2ê°œ ì´ìƒì˜ ë³€ìˆ˜ë¥¼ ê°€ì§€ëŠ” í•¨ìˆ˜ë¥¼ ë§í•œë‹¤.

í”„ë¡œê·¸ë¨ì ìœ¼ë¡œ êµ¬í˜„í•˜ì—¬ ë‹¤ë°©ë©´ìœ¼ë¡œ í™œìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ë³€ìˆ˜ê°€ ëª‡ê°œë“  ì‚¬ìš©í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤.

> `nditer` : ë°˜ë³µì ê°ì²´
>
> í•˜ë‚˜ ì´ìƒì˜ ë°°ì—´ì˜ ëª¨ë“  ìš”ì†Œë¥¼ ë°©ë¬¸í•  ìˆ˜ ìˆëŠ” ì—¬ëŸ¬ê°€ì§€ ìœ ì—°í•œ ë°©ë²•ì„ ì œê³µí•œë‹¤.
>
> [ref](https://runebook.dev/ko/docs/numpy/reference/arrays.nditer)



```python
# ë‹¤ë³€ìˆ˜ í•¨ìˆ˜
# f(x,y) = 2x + 3xy + y^3
# f(a, b, c) = 3a + 3bc + b^2 + c^3

import numpy as np

def my_func(input_value):
    x = input_value[0]
    y = input_value[1]
    return 2 * x + 3 * x * y + y ** 3

result = numerical_derivative(my_func, np.array([1.0, 2.0]))
print(result)   # [ 8.         15.00000001]
```



```python
# ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì— ëŒ€í•œ ìˆ˜ì¹˜ë¯¸ë¶„ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜
def numerical_derivative(f, x):   # x   : [1.0  2.0]
                                  # ê²°ê³¼ : [8.0 15.0]
    delta_x = 1e-4
    derivative_x = np.zeros_like(x)  # derivative_x : [0.0 0.0] ê°’ ì €ì¥
    
    # iterator ë¥¼ ì´ìš©í•´ì„œ ì…ë ¥ë³€ìˆ˜ xì— ëŒ€í•œ í¸ë¯¸ë¶„ì„ ìˆ˜í–‰
    it = np.nditer(x, flags=['multi_index'])
    
    while not it.finished:
        
        idx = it.multi_index
        tmp = x[idx]
    
        x[idx] = tmp + delta_x   # x : ndarray [1.0001 2.0]
        fx_plus_delta = f(x)
        
        x[idx] = tmp -delta_x    # x : ndarray [0.9999 2.0]
        fx_minus_delta = f(x)
        
        derivative_x[idx] = (fx_plus_delta - fx_minus_delta) / (2*delta_x)
        
        x[idx] = tmp             # x : ndarray [1.0  2.0]
        it.iternext()
        
    return derivative_x
```



ë‹¤ë¥¸ í•¨ìˆ˜ë¡œ í™•ì¸

```python
# ì •ìƒì ìœ¼ë¡œ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ ë‹¤ë¥¸ 4ë³€ìˆ˜ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ test

# f(x, w, y, z) = wx + xyz + 3w + zy^2
# ìˆ˜ì¹˜ë¯¸ë¶„
# f'(1.0, 2.0, 3.0, 4.0) =?

def my_func(input_value):
    
    w = input_value[0,0]
    x = input_value[0,1]
    y = input_value[1,0]
    z = input_value[1,1]
    
    return (w*x) + (x*y*z) + (3*w) + (z*(y**2))

result = numerical_derivative(my_func, np.array([[1.0, 2.0], [3.0, 4.0]]))
print(result)
```

ì•„ì•„ì•„ì•„ ì–´ë ¤ì›Œ ã… ã… 




### 04. Regression (íšŒê·€)

##### A) íšŒê·€ë€?

- ì–´ë–¤ ë°ì´í„°ì— ëŒ€í•´ ê·¸ ë°ì´í„°ì— ì˜í–¥ì„ ì£¼ëŠ” ì¡°ê±´ë“¤ì˜ ì˜í–¥ë ¥ì„ ê³ ë ¤í•´ì„œ ë°ì´í„°ì— ëŒ€í•œ ì¡°ê±´ë¶€ í‰ê· ì„ êµ¬í•˜ëŠ” ê¸°ë²•

- ì¡°ê±´ì— ë”°ë¥¸ ê°€ê²©ì˜ ë³€í™”ë¥¼ ê·¸ë˜í”„ë¡œ ê·¸ë¦¬ê³ , ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ í™œìš©ë„ê°€ ì»¤ì§„ë‹¤.
Machine learning

# Machine learning



### 01. Log

##### A) 로그 표현방식

Log는 지수를 다른 방법으로 표현한다.

> 3<sup>x</sup> = 81, x = 4 (지수방정식)
>
> log<sub>3</sub> 81 = x, x = 4 (로그방정식)

여기서 `3`은 밑, `81`은 진수 `x`는 지수라고 한다.

지수방정식은 진수가 분리된형태이고, 로그방정식은 지수가 분리된 형태이다.

##### B) 로그 종류

- 상용로그 (common logarithm) : 십진로그

  밑이 10인 로그 

  > log<sub>10</sub> (x) = log(x)

- 자연로그 (natural logarithm) : 자연로그

  밑이 자연상수 e (=2.718...) 인 로그

  > log<sub>e</sub> (x) = ln (x)

##### C) 로그를 사용하는 이유

- 수학

  지수방정식을 쉽게 해결하기 위해서 로그방정식을 이용한다.

- 머신러닝

  정규성을 높이기 위해서 사용한다.

  > 1000 ☞ log<sub>10</sub> 1000 =  log<sub>10</sub> 10<sup>3</sup> = 3
  >
  > 100000000 ☞ log<sub>10</sub> 10<sup>8</sup> = 8

### 02. derivative (미분)

##### A) 미분 개요

- 미분의 정의

  \- 어떤 함수의 정의역 속 각 점에서 독립변수 값의 변화량과 함수값으 ㅣ변화량의 비율의 극한, 극한의 집합을 치역으로 가지는 새로운 함수

  \- 함수에 대해서 특정 순간의 변화량

- 미분의 종류

  - 해석미분 : 종이와 펜을 이용해 논리적인 전개로 수행하는 미분
  - 수치미분 : 해석미분으로 해결할 수 없을 때 수치값을 이용해 미분의 근사값을 알아내는 방법

##### B)  미분

미분은 함수에 대해서 특정 순간의 변화량을 나타낸다.

즉 x에서의 작은 변화가 f(x)를 얼마나 변화시키는지를 나타낸다.

미분을 수식으로 알아보자

<table>
      <tr>
        <th style="text-align:center;"><img src="../img/image-20220325174802786.png" width="500" height="350"></th>
        <td >
            여기서 직선의 기울기가 비율이다.<br>
            만약 △x ⇒ 0 으로 0에 가까워지게 되면<br>
            x위치에서 f(x)의 접선이 된다.<br>
            이 기울기가 미분값(변화량)이다.
         </td>  
      </tr>
</table>

$$
\lim_{x \to 0} {f(x+\Delta x) - f(x)\over \Delta x}
$$

##### C) 수치미분

수치미분의 종류로는 **전향미분, 후향미분, 중앙차분**이 있으며 그 중 <span style="color:red">중앙차분</span>이 가장 값이 정확하기 때문에 많이 사용된다.

미분과 의미는 똑같으며, x의 좌우로 △x만큼 떨어진 곳으로 부터 계산한 값을 사용한다.

<table>
      <tr>
        <th style="text-align:center;"><img src="../img/image-20220325174827447.png" width="480" height="350"></th>
        <td >
            △x ⇒ 0 으로 0에 가까워지게 되면<br>
            x위치에서 f(x)의 접선이 된다.<br>
            이 기울기가 미분값(변화량)이다.
         </td>  
      </tr>
</table>

$$
\lim_{x \to 0} {f(x+\Delta x) - f(x-\Delta x)\over 2\Delta x}
$$

> 📌 프로그램에서 △x
>
> 프로그램으로 미분을 구현 할 때에 △x는 어떻게 해야할까?
>
>  △x는 0에 가까운 아주 작은 값이므로 0.000...1 의값으로 사용한다.
>
> 하지만 프로그래밍적으로 10<sup>-8</sup>이하의 값을 사용한다면 문제가 발생하기 때문에
>
> 이번 프로그래밍 실습에서는 △x값을 10<sup>-4</sup>로 설정하였다.



##### D) 기본 미분 공식

- f(x) = constant

  f'(x) = 0

- f(x) = ax<sup>n</sup>

  f'(x) = n·ax<sup>n-1</sup>

- f(x) = e<sup>x</sup>

  f'(x) = e<sup>x</sup>

- f(x) = e<sup>-x</sup>

  f'(x) = -e<sup>-x</sup>

- f(x) = ln(x) = log<sub>e</sub>(x)

  f'(x) = 1/x

##### E) 편미분 (partial deviation)

독립변수가 2개 이상인 다변수함수에서 미분하고자 하는 변수 하나를 제외한 나머지 변수를 상수로 취급해서 해당 변수를 미분하는 방법
$$
f(x,y) = 2x + 3xy + y^3
\\\\
{\delta f(x,y)  \over \delta x} = 2 + 3y
\\
{\delta f(x,y)  \over \delta y} = 3x + 3y^2
$$

##### F) 미분의 Chain Rule (연쇄법칙)

여러 함수로 구성된 합성함수를 미분할 때 사용
$$
f(x) = 3^{3x^2}
\\
\text{function1 : } e^t
\\
\text{function2 : } t = 3x^2
\\\\
{\delta f(x)\over \delta x} = {\delta f\over \delta t}\times {\delta t\over \delta x}
\\\\
f'(x) = e^t \times 6x = 6xe^{3x^2}
$$


### 03. 미분 구현

##### A) first-classes function

한국어로는 일급함수라고 하며 javascript, python에서 이를 지원한다.

일급함수는 함수가 변수에 저장될 수 있어 함수이름을 변수처럼 사용할 수 있다.

```python
def my_func3(x):
    x(20)
    
def my_func4(x):
    print(x+30)

my_func3(my_func4) # 50
```

##### B) 단변수 미분 함수

다음 중앙차분 공식을 프로그램으로 구현해보자.
$$
\lim_{x \to 0} {f(x+\Delta x) - f(x-\Delta x)\over 2\Delta x}
$$

```python
# 단변수 함수
# f(x) = x^2
# f'(x) = 2x
# f'(5) = 10

# f(x)
def my_func(x):
    return x**2

# 미분을 수행하는 함수
def numerical_derivative(f, x):
    
    delta_x = 1e-4 # 극한에 해당하는 값. 너무 작은 값을 사용하면 실수계산 오류가 발생해요.
                   # 1e-4 정도의 값을 이용하면 적당한 수치미분 값을 구할 수 있어요.
    return (f(x + delta_x) - f(x-delta_x)) / (2*delta_x)
    
    
# f'(5)
result = numerical_derivative(my_func, 5)
print(result)    # 9.999999999976694 약간 차이가 있는 근사값을 얻어낼 수 있다.
```

##### C) 다변수 미분함수

다변수 함수는 2개 이상의 변수를 가지는 함수를 말한다.

프로그램적으로 구현하여 다방면으로 활용하기 위해서는 변수가 몇개든 사용할 수 있어야 한다.

> `nditer` : 반복자 객체
>
> 하나 이상의 배열의 모든 요소를 방문할 수 있는 여러가지 유연한 방법을 제공한다.
>
> [ref](https://runebook.dev/ko/docs/numpy/reference/arrays.nditer)



```python
# 다변수 함수
# f(x,y) = 2x + 3xy + y^3
# f(a, b, c) = 3a + 3bc + b^2 + c^3

import numpy as np

def my_func(input_value):
    x = input_value[0]
    y = input_value[1]
    return 2 * x + 3 * x * y + y ** 3

result = numerical_derivative(my_func, np.array([1.0, 2.0]))
print(result)   # [ 8.         15.00000001]
```



```python
# 다변수 함수에 대한 수치미분을 수행하는 함수
def numerical_derivative(f, x):   # x   : [1.0  2.0]
                                  # 결과 : [8.0 15.0]
    delta_x = 1e-4
    derivative_x = np.zeros_like(x)  # derivative_x : [0.0 0.0] 값 저장
    
    # iterator 를 이용해서 입력변수 x에 대한 편미분을 수행
    it = np.nditer(x, flags=['multi_index'])
    
    while not it.finished:
        
        idx = it.multi_index
        tmp = x[idx]
    
        x[idx] = tmp + delta_x   # x : ndarray [1.0001 2.0]
        fx_plus_delta = f(x)
        
        x[idx] = tmp -delta_x    # x : ndarray [0.9999 2.0]
        fx_minus_delta = f(x)
        
        derivative_x[idx] = (fx_plus_delta - fx_minus_delta) / (2*delta_x)
        
        x[idx] = tmp             # x : ndarray [1.0  2.0]
        it.iternext()
        
    return derivative_x
```



다른 함수로 확인

```python
# 정상적으로 동작하는지 확인하기 위해 다른 4변수 함수를 사용하여 test

# f(x, w, y, z) = wx + xyz + 3w + zy^2
# 수치미분
# f'(1.0, 2.0, 3.0, 4.0) =?

def my_func(input_value):
    
    w = input_value[0,0]
    x = input_value[0,1]
    y = input_value[1,0]
    z = input_value[1,1]
    
    return (w*x) + (x*y*z) + (3*w) + (z*(y**2))

result = numerical_derivative(my_func, np.array([[1.0, 2.0], [3.0, 4.0]]))
print(result)
```

아아아아 어려워 ㅠㅠ




### 04. Regression (회귀)

##### A) 회귀란?

- 어떤 데이터에 대해 그 데이터에 영향을 주는 조건들의 영향력을 고려해서 데이터에 대한 조건부 평균을 구하는 기법

- 조건에 따른 가격의 변화를 그래프로 그리고, 수식으로 표현하면 활용도가 커진다.

##### B) Regression Model

- 여러가지 복잡한 조건하에서 평균을 집계할 수 있도록 하는 효율적인 방법

- 조건들을 이용해 평균을 추정할 수 있도록 하는 기법

  - 평균을 구할 때 주의해야할 사항이 regression에도 적용

    \- 정규분포를 띄는 데이터를 사용해야한다

- 회귀모델

  - 식 : 영향력<sub>1</sub> x 조건<sub>1</sub> + 영향력<sub>2</sub> x 조건<sub>2</sub> ... + 영향력<sub>n</sub> x 조건<sub>n</sub>
  - 정규분포이며, outlier가 없어야 한다.
  - 식에 포함된 영향력은 회귀계수(regression coefficient)라고 한다.

- 고전적 회귀모델(Classical Linear Regression Model)

  - 단순 선형회귀 (simple linear regression)
  - 다중 선형회귀 (multiple linear regression)

$$
\hat y = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + ... \beta_{n}x_{n}
\\
\hat y = \beta_{0} + \sum_{i=1}^p \beta_{i}x_{i}
$$

### 05. 단순 선형회귀

단순 선형회귀는 독립변수가 하나인 선형회귀모델이다.

위의 모델식이 독립변수가되면 이렇게 표현될 수 있다.
$$
y = Wx + b
$$
여기서 W는 weight로 가중치를 뜻하며, b는 bias로 바이어스라고 칭한다.

수학에서 배웠던 직선방정식과 동일한 식이다.

이제 이 식에서 데이터를 잘 나타낼 수 있는 W, b를 찾아야 한다.

##### 과정1 : learning

처음에는 w, b값을 랜덤으로 주어 표현한 다음, 데이터를 잘 표현하는 a, b값을 계속 찾아나가는 learning 방식을 사용한다.

<table>
      <tr>
        <th style="text-align:center;"><img src="../img/image-20220329135849392.png" width="480" height="350"></th>
        <td >
            처음 값은 random으로 시작한다.<br>
            1차, 2차 수정을 거쳐 데이터를 잘 나타내는 직선을 찾는다.<br>
         </td>  
      </tr>
</table>


##### 과정2 : MSE

만들어진 직선이 어떻게 데이터와 맞는지 알 수 있을까? 

실제값(t)와 예측값(y)의 차이인 <span style="background-color:#fff5b1;">오차</span>를 사용해서 얼마나 오차가나는지 확인할 수 있다.
$$
\text{Error} = t-y = t-(Wx+b)
$$
각 값마다 오차값이 있으니, 이 값을 통일해줄 무언가가 필요하다.

여기서 <span style="background-color:#fff5b1;">MSE(Mean Squared Error) 평균제곱오차</span>를 사용해서 확인한다.

MSE는 부호가 뒤죽박죽인 오차들의 절대값의 평균을 내는 방법이다.

- MSE를 사용해서 오차에 대한 가중치를 줄 수 있다.

- MSE는 작으면 작을수록 좋다.

$$
MSE\quad = \quad{(t_{1}-y_{1})^2 + (t_{2}-y_{2})^2 + (t_{3}-y_{3})^2 ... + (t_{n}-y_{n})^2\over n}
$$

<table>
      <tr>
        <th style="text-align:center;"><img src="../img/image-20220329135905113.png" width="480" height="350"></th>
        <td >
            만들어진 회귀모델과 실제값 사이의 차를 구하면 오차를 알 수 있다.<br>
            값당 하나씩 있는 오차를 하나로 표현하는 방법 중 하나로 MSE가 있다.<br>
     </td>  
  </tr>
</table>


##### 과정3 : loss function

각 값의 오차를 사용해서 만든 값인 <span style="background-color:#fff5b1;">MSE</span> 는 작을수록 오차가 작다는 의미이므로 작은 MSE 값을 찾는것을 목적으로 한다.

작은 MSE 값을 찾는 문제를 <span style="background-color:#fff5b1;">Loss Function(손실함수), Cost Function(비용함수)</span> 라고한다.

loss function은 W, b의 함수이다.
$$
E(w,b) = \quad{(t_{1}-y_{1})^2 + (t_{2}-y_{2})^2 + (t_{3}-y_{3})^2 ... + (t_{n}-y_{n})^2\over n}
\\
= \quad{(t_{1}-(wx_{1}+b))^2 + (t_{2}-(wx_{2}+b))^2 + ... (t_{n}-(wx_{n}+b))^2\over n}
\\
= \quad{1\over n}\sum_{i=1}^n[t_{i}-(wx_{i}+b)]^2
$$

<table>
      <tr>
        <th style="text-align:center;"><img src="../img/image-20220329135930174.png" width="480" height="350"></th>
        <td >
            실제값을 이용해서 loss function의 모양을 확인해보자<br>
            loss의 값이 최소가 되게하는 W를 찾아야 내가 가진 데이터와 가장 가까운 직선을 만들 수 있다.<br>
     </td>  
  </tr>
</table>

##### 과정4 : 경사하강법

데이터와 가장 가까운 직선을 만들려면 loss function의 값이 최소가 되게하는 W를 구해야 한다.

<span style="background-color:#fff5b1;">Gradient Descent Algorithm(경사하강법)</span> 을 사용해서 loss function의 값이 최소가 되게 하는 W를 구할 수 있다.

1. w 값을 랜덤하게 설정한다.
2. w에서 편미분해서 값을 조사한다.
3. w = w - a*편미분값
   - 현재 w 값에서 편미분한 값을 빼준다.
   - 편미분한 값이 클 수 있으니, learning rate(학습율) a를 설정해서 곱한다.
   - loss 함수가 최소가 될 때 까지 움직이면서 조사한다.

<table>
      <tr>
        <th style="text-align:center;"><img src="../img/image-20220329135949465.png" width="480" height="350"></th>
        <td >
            w를 random하게 잡아서 그 때의 기울기를 계산한다.<br>
            미분을 통해 기울기를 계산하고, 그 값에 a를 곱해 w에서 빼준다.<br>
            이것을 반복적으로 해주면 접선의 기울기가 0에 도달할 수 있을것이다.
     </td>  
  </tr>
</table>

### 06. 단순 선형회귀 구현

##### A) python으로 구현

```python
import numpy as np

# Training Data Set 준비
# x_data : 독립변수(공부시간)
x_data = np.array([1, 2, 3, 4, 5], dtype =np.float64).reshape(5,1) # reshape(5,-1) : 계산안할테니까 알아서 채워
# t_data => 정답데이터(시험점수)
t_data = np.array([3, 5, 7, 9, 11], dtype =np.float64).reshape(5,1)

# weight & bias 정의
W = np.random.rand(1,1)    # 1행 1열짜리 (값은1개) ndarray를 만들고
                           # 0과 1사이 균등분포에서 실수 난수를 추출
b = np.random.rand(1)

# predict function (예측 함수, 모델, hypothesis)
def predict(x):
    y = np.dot(x,W) + b
    
    return y


# loss function
def loss_func(input_data):     # loss 함수는 w와 b의 함수
                                 # input data => [W,b]
        input_W = input_data[0].reshape(1,1)
        input_b = input_data[1]
        
        # 예측값
        y = np.dot(x_data, input_W) + input_b
        
        # MSE를 구해요 (평균제곱오차)
        return np.mean(np.power(t_data-y,2))
    
# 수치미분    
def numerical_derivative(f, x):   # x    : [1.0  2.0]
                                            # 결과 : [8.0 15.0]
    delta_x = 1e-4
    derivative_x = np.zeros_like(x) # derivative_x : [0.0 0.0]
    
    # iterator 를 이용해서 입력변수 x에 대한 편미분을 수행
    it = np.nditer(x, flags=['multi_index'])
    
    while not it.finished:
        
        idx = it.multi_index
        tmp = x[idx]
    
        x[idx] = tmp + delta_x   # x : ndarray [1.0001 2.0]
        fx_plus_delta = f(x)
        
        x[idx] = tmp -delta_x    # x : ndarray [0.9999 2.0]
        fx_minus_delta = f(x)
        
        derivative_x[idx] = (fx_plus_delta - fx_minus_delta) / (2*delta_x)
        
        x[idx] = tmp             # x : ndarray [1.0  2.0]
        it.iternext()
        
    return derivative_x    

# learning rate
learning_rate = 1e-4

# 학습과정 진행
for step in range(300000):
    input_param = np.concatenate((W.ravel(), b.ravel()), axis=0) # [W b]
    derivate_result = learning_rate * numerical_derivative(loss_func, input_param)
    
    W = W - derivate_result[0].reshape(1,1)
    b = b - derivate_result[1]
    
    if step % 30000 == 0 :
        input_param = np.concatenate((W.ravel(), b.ravel()), axis=0)
        print('W:{}, b:{}, loss:{}'.format(W, b, loss_func(input_param)))
```

> W:[[0.94177277]], b:[0.99407781], loss:12.355930885525364
> W:[[1.97320749]], b:[1.09672946], loss:0.001703062723414769
> W:[[1.99028359]], b:[1.03507934], loss:0.00022398340309209348
> W:[[1.9964763]], b:[1.01272166], loss:2.945784918605578e-05
> W:[[1.99872212]], b:[1.00461356], loss:3.8742374063799574e-06
> W:[[1.99953657]], b:[1.00167313], loss:5.0953195483918e-07
> W:[[1.99983194]], b:[1.00060677], loss:6.701262358673841e-08
> W:[[1.99993905]], b:[1.00022005], loss:8.813366222411097e-09
> W:[[1.9999779]], b:[1.0000798], loss:1.1591162983377238e-09
> W:[[1.99999198]], b:[1.00002894], loss:1.5244465721056137e-10

```python
# 6시간 공부하면 몇점일지를 예측해보아요

predict_date = predict(np.array([[6]]))   # 1행 1열짜리 2차원 matrix로 만들어줌
print('6시간 공부했을 때 점수 : ',predict_date)
```

> 6시간 공부했을 때 점수 :  [[12.99999305]]

> 📌 반복학습
>
> 학습과정의 range 크기가 클수록 시간이 오래걸리고 overfitting될 수 있다.
>
> 몇번 반복학습할 것인지도 잘 조정해 주어야 한다.

##### B) sklearn으로 구현

```powershell
> pip install sklearn
```

```python
import numpy as np
from sklearn import linear_model

# Training Data Set 준비
x_data = np.array([1, 2, 3, 4, 5], dtype =np.float64).reshape(5,1)
t_data = np.array([3, 5, 7, 9, 11], dtype =np.float64).reshape(5,1)

# model 생성 (Simple Linear Model)
model = linear_model.LinearRegression()

# model 학습
model.fit(x_data, t_data)

# 학습이 끝났으니.. 구한 W 와 b를 출력해보아요
print('W:{},b:{}'.format(model.coef_, model.intercept_))

# model을 이용한 예측
model.predict(np.array([[6]]))
```

